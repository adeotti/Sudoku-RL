{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeotti/sudoku-env/blob/main/M9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_VxUuPOyB90G"
      },
      "outputs": [],
      "source": [
        "# python version 3.9.11 \n",
        "%pip install numpy==1.25.2 tensorboard==2.17.0 torchrl==0.4.0 gymnasium==0.29.1 tensordict==0.4.0\n",
        "\n",
        "from IPython.display import clear_output\n",
        "def clear():\n",
        "  clear_output(wait=False)\n",
        "\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "import math,sys \n",
        "import torch\n",
        "\n",
        "clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "easyBoard = torch.tensor([\n",
        "    [0, 0, 0, 5, 3, 1, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 4, 0, 3, 0, 1],\n",
        "    [1, 0, 0, 8, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 4, 0, 0, 5, 6, 0, 0],\n",
        "    [0, 0, 3, 9, 0, 2, 1, 4, 0],\n",
        "    [6, 1, 5, 0, 7, 0, 0, 9, 8],\n",
        "    [0, 2, 0, 0, 9, 6, 0, 1, 0],\n",
        "    [0, 5, 7, 2, 0, 8, 0, 0, 6],\n",
        "    [0, 6, 1, 7, 5, 3, 0, 2, 4]])\n",
        "\n",
        "solution = torch.tensor([\n",
        "    [8, 4, 9, 5, 3, 1, 7, 6, 2],\n",
        "    [5, 7, 2, 6, 4, 9, 3, 8, 1],\n",
        "    [1, 3, 6, 8, 2, 7, 4, 5, 9],\n",
        "    [2, 9, 4, 1, 8, 5, 6, 7, 3],\n",
        "    [7, 8, 3, 9, 6, 2, 1, 4, 5],\n",
        "    [6, 1, 5, 3, 7, 4, 2, 9, 8],\n",
        "    [3, 2, 8, 7, 9, 5, 1, 6, 7],\n",
        "    [4, 5, 7, 2, 1, 8, 9, 3, 6],\n",
        "    [9, 6, 1, 7, 5, 3, 8, 2, 4]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lGSDUcbPH4KM"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Board_specs:\n",
        "  size: tuple = (9,9)\n",
        "  low: int = 1\n",
        "  high: int = 9\n",
        "\n",
        "def Board():\n",
        "  matrix = easyBoard\n",
        "  return matrix\n",
        "\n",
        "def Region_Split(matrix):\n",
        "  # Extract the 3X3 regions\n",
        "    ZERO = 0 ; CONST = 3 ; MAX_COLUMN_END = 12\n",
        "    row_start = col_start = ZERO\n",
        "    row_end = col_end = CONST\n",
        "    subgrid = []\n",
        "    while len(subgrid) < len(matrix) :\n",
        "        subgrid.append([row[col_start:col_end] for row in matrix[row_start:row_end]])\n",
        "        col_start += CONST\n",
        "        col_end += CONST\n",
        "        if col_end == MAX_COLUMN_END :\n",
        "            col_start = ZERO\n",
        "            col_end = CONST\n",
        "            row_start += CONST\n",
        "            row_end += CONST\n",
        "    assert len(subgrid) == len(matrix)\n",
        "    subgrid = [torch.concatenate(line) for line in subgrid]\n",
        "    return subgrid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A8GSYA58P6SD"
      },
      "outputs": [],
      "source": [
        "class Game:\n",
        "    def __init__(self,action = None):\n",
        "        self.board = Board()\n",
        "        self.action = action\n",
        "\n",
        "        self.modifiableCells = []\n",
        "        \n",
        "        for i,x in enumerate(self.board):\n",
        "            for y in range(Board_specs.high):\n",
        "                if x[y] == 0:\n",
        "                    self.modifiableCells.append((i,y))            \n",
        "\n",
        "    def Updated_board(self):\n",
        "        if self.action is not None:\n",
        "            self.action = self.action.int() #.tolist()[0] #tensor([[0, 1, 0]], dtype=torch.int32)\n",
        "            row,column,value = self.action\n",
        "            \n",
        "            if (row,column) in self.modifiableCells:\n",
        "                self.board[row][column] = value\n",
        "                \n",
        "        return self.board,self.action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K9VeBJ-ZQBjj"
      },
      "outputs": [],
      "source": [
        "class Util:\n",
        "    @classmethod\n",
        "    def Utilities(cls,gameinstance):\n",
        "        if gameinstance is None :\n",
        "            sys.exit(f\"{cls.__name__} : No Game instance was provided\")\n",
        "        cls.upd_matrix,cls.action = gameinstance.Updated_board()\n",
        "\n",
        "        # TODO : just transpose the matrix then ylist will be easy to implement\n",
        "        cls.y_list = [[col[y_index] for col in cls.upd_matrix] for y_index in range(Board_specs.high)]\n",
        "        cls.y = [line for line in torch.t(cls.upd_matrix)]\n",
        "        cls.subgrid = Region_Split(cls.upd_matrix)\n",
        "        cls.subgrid = [arr.tolist() for arr in cls.subgrid]\n",
        "\n",
        "    @staticmethod\n",
        "    def unique(lisst):\n",
        "        return len(lisst) == len(set(lisst))\n",
        "\n",
        "    @staticmethod\n",
        "    def len_dif(lisst):\n",
        "        return len(lisst) - len(set(lisst))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reward Function "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tOk1m0EZQKX2"
      },
      "outputs": [],
      "source": [
        "class boardConflicts: # Heuristic component\n",
        "\n",
        "    previous_conflicts = 0\n",
        "\n",
        "    def __init__(self,instance):\n",
        "        Util.Utilities(instance)\n",
        "        self.conflicts = 0\n",
        "        self.previous_conflicts = 0\n",
        "        self.reward_H = 0\n",
        "\n",
        "    def count(self):\n",
        "        for line in Util.upd_matrix:\n",
        "            if not Util.unique(line) : self.conflicts += Util.len_dif(line)\n",
        "        for col in Util.y_list:\n",
        "            if not Util.unique(col) : self.conflicts += Util.len_dif(col)\n",
        "        for submatrix in Util.subgrid:\n",
        "            if not Util.unique(submatrix) : self.conflicts += Util.len_dif(submatrix)\n",
        "        # H value\n",
        "        if self.conflicts < self.previous_conflicts:\n",
        "            self.reward_H = 2\n",
        "        boardConflicts.previous_conflicts = self.conflicts\n",
        "        return self.reward_H\n",
        "    \n",
        "\n",
        "class rewardFunction:\n",
        "    def __init__(self,instance):\n",
        "        Util.Utilities(instance)\n",
        "        self.action = Util.action\n",
        "        self.matrix = Util.upd_matrix\n",
        "        self.y_list = Util.y_list\n",
        "        self.reward = 0\n",
        "\n",
        "    def rewardReturns(self):\n",
        "        if self.action is None :\n",
        "            sys.exit(f\"{self.__class__.__name__} : Action is None\")\n",
        "        # x\n",
        "        x = self.matrix[self.action[0]]#; assert len(x) == Board_specs.high\n",
        "        self.reward += 9 if (Util.len_dif(x) == 0) else - Util.len_dif(x)\n",
        "        # y\n",
        "        y_index = self.action[1]\n",
        "        y = self.y_list[y_index]; assert len(y) == Board_specs.high\n",
        "        self.reward += 9 if (Util.len_dif(y) == 0) else -Util.len_dif(y)\n",
        "        # region\n",
        "        n = int(math.sqrt(Board_specs.high))\n",
        "        ix,iy = (self.action[0]//n)* n , (self.action[1]//n)* n\n",
        "        region = torch.flatten(self.matrix[ix:ix+n , iy:iy+n])\n",
        "        assert len(region) == Board_specs.high\n",
        "        self.reward += 9 if (Util.len_dif(region) == 0) else  -Util.len_dif(region)\n",
        "        return self.reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Rjuo5ittQZfI"
      },
      "outputs": [],
      "source": [
        "def gameEnd(instance):\n",
        "    Util.Utilities(instance)\n",
        "    matrix = Util.upd_matrix\n",
        "    return torch.equal(solution,matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorDict(\n",
            "    fields={\n",
            "        action: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.int32, is_shared=False),\n",
            "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "        next: TensorDict(\n",
            "            fields={\n",
            "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
            "                observation: Tensor(shape=torch.Size([9, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
            "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "            batch_size=torch.Size([]),\n",
            "            device=None,\n",
            "            is_shared=False),\n",
            "        observation: Tensor(shape=torch.Size([9, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
            "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
            "    batch_size=torch.Size([]),\n",
            "    device=None,\n",
            "    is_shared=False)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorDict(\n",
              "    fields={\n",
              "        action: Tensor(shape=torch.Size([1, 3]), device=cpu, dtype=torch.int32, is_shared=False),\n",
              "        done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
              "        next: TensorDict(\n",
              "            fields={\n",
              "                done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
              "                observation: Tensor(shape=torch.Size([1, 9, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
              "                reward: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.int64, is_shared=False),\n",
              "                terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
              "            batch_size=torch.Size([1]),\n",
              "            device=None,\n",
              "            is_shared=False),\n",
              "        observation: Tensor(shape=torch.Size([1, 9, 9]), device=cpu, dtype=torch.float32, is_shared=False),\n",
              "        terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
              "    batch_size=torch.Size([1]),\n",
              "    device=None,\n",
              "    is_shared=False)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchrl.envs import EnvBase,Transform,TransformedEnv\n",
        "from torchrl.data import BoundedTensorSpec,CompositeSpec,OneHotDiscreteTensorSpec,DiscreteTensorSpec,CompositeSpec\n",
        "from tensordict import TensorDictBase,TensorDict\n",
        "\n",
        "class environment(EnvBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.action = None\n",
        "        self.game = Game(self.action)\n",
        "        self.updatedBoard,_ = self.game.Updated_board()\n",
        "\n",
        "        # specs\n",
        "        \"\"\"self.action_spec = OneHotDiscreteTensorSpec(\n",
        "            n=9,\n",
        "            shape=torch.Size([3,9]),\n",
        "            dtype=torch.int\n",
        "        )\"\"\"\n",
        "\n",
        "        x_specs = BoundedTensorSpec(low=0,high=9,shape=(1,),dtype=torch.int)\n",
        "        y_specs =   BoundedTensorSpec(low=0,high=9,shape=(1,),dtype=torch.int)\n",
        "        value_specs =   BoundedTensorSpec(low=1,high=9,shape=(1,),dtype=torch.int)\n",
        "\n",
        "        self.action_spec = CompositeSpec(x = x_specs,y = y_specs, v = value_specs, shape=(1,))\n",
        "        #TODO  work to be done here\n",
        "\n",
        "\n",
        "        self.observation_format = BoundedTensorSpec(\n",
        "            low=1.0,\n",
        "            high=9.0,\n",
        "            shape=(easyBoard).unsqueeze(0).shape,\n",
        "            dtype=torch.float32\n",
        "            )\n",
        "        self.observation_spec = CompositeSpec(observation = self.observation_format) \n",
        "        \n",
        "    def _step(self,tensordict) -> TensorDictBase :\n",
        "        tensordict.update({\"action\" : torch.tensor([tensordict[\"x\"],tensordict[\"y\"],tensordict[\"v\"]])})\n",
        "         \n",
        "        self.action = tensordict[\"action\"]\n",
        "        self.updated,_ = Game(self.action).Updated_board()\n",
        "\n",
        "        self.game = Game(self.action)\n",
        "        reward = rewardFunction(self.game).rewardReturns()\n",
        "\n",
        "        output = TensorDict({\n",
        "                            \"observation\" :  self.updatedBoard.clone().detach().float(),\n",
        "                             \"reward\" : torch.tensor(reward),\n",
        "                             \"done\" : gameEnd(self.game)})\n",
        "        tensordict.pop(\"x\")\n",
        "        tensordict.pop(\"y\")\n",
        "        tensordict.pop(\"v\")\n",
        "        return output\n",
        "        \n",
        "    def _reset(self,tensordict = None,**kwargs) -> TensorDictBase :\n",
        "        output = TensorDict({\"observation\" : self.updatedBoard.clone().detach().float()})\n",
        "        return output\n",
        "\n",
        "    def _set_seed(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "rolloutTest = environment()\n",
        "\"\"\"maxSteps = 2\n",
        "container = []\n",
        "x = []\n",
        "for element in rolloutTest.rollout(maxSteps):\n",
        "    container.append(element[\"next\",\"observation\"])\"\"\"\n",
        "\n",
        "\n",
        " \n",
        "\"\"\"\n",
        "for element in x :\n",
        "    print(torch.argmax(element))\n",
        "\"\"\"\n",
        "\n",
        "rolloutTest.rollout(1)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Mmh6rSNIIe",
        "outputId": "94c5da36-583b-4f5b-e402-6d941992511d"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import OneHotCategorical\n",
        "\n",
        "from tensordict.nn import TensorDictModule\n",
        "\n",
        "from torchrl.modules import ValueOperator,ProbabilisticActor\n",
        "from torchrl.objectives.value import GAE\n",
        "from torchrl.objectives import ClipPPOLoss\n",
        "\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data import ReplayBuffer,SamplerWithoutReplacement,LazyTensorStorage\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hypers\n",
        "l_rate = 0.01\n",
        "sdg_momentum = 0.9\n",
        "\n",
        "frames =  10             # number of steps\n",
        "sub_frame = 5              # for the most inner loop of the training step\n",
        "total_frames = 20      # maximum steps\n",
        "epochs = 10\n",
        "\n",
        "gamma = 0.80\n",
        "lmbda = 0.99\n",
        "\n",
        "env = environment()\n",
        "\n",
        "dummy_observation = env._reset()[\"observation\"].unsqueeze(0)\n",
        "action_spec = tuple(env.action_spec.shape) # (3,9)\n",
        "action_dist = env.action_spec.shape.numel() # 27\n",
        "size = torch.flatten(dummy_observation).shape.numel() # 81\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def weights_init(w):\n",
        "  if isinstance(w,(nn.Conv2d,nn.LazyConv2d,nn.LazyLinear)):\n",
        "    nn.init.kaiming_uniform(w.weight,mode=\"fan_in\",nonlinearity=\"relu\")\n",
        "    if w.bias is not None : nn.init.zeros_(w.bias)\n",
        "\n",
        "def Network_util(network : nn.Module):\n",
        "  network.to(device)\n",
        "  network.forward(dummy_observation)\n",
        "  network.apply(weights_init)\n",
        "  return network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "g3Sw6-yR8kH4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\14385\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "spec keys and out_keys do not match, got: {'action', 'y', 'v', 'sample_log_prob', 'x'} and {'action', 'sample_log_prob'} respectively",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m ActorNetwork()\u001b[38;5;241m.\u001b[39mforward(dummy_observation)\n\u001b[0;32m     25\u001b[0m Policy \u001b[38;5;241m=\u001b[39m TensorDictModule(module\u001b[38;5;241m=\u001b[39mActorNetwork(), in_keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m\"\u001b[39m],out_keys\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 26\u001b[0m PolicyModule \u001b[38;5;241m=\u001b[39m \u001b[43mProbabilisticActor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPolicy\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43min_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdistribution_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mOneHotCategorical\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreturn_log_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\14385\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchrl\\modules\\tensordict_module\\actors.py:301\u001b[0m, in \u001b[0;36mProbabilisticActor.__init__\u001b[1;34m(self, module, in_keys, out_keys, spec, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28mlen\u001b[39m(out_keys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec, CompositeSpec)\n\u001b[0;32m    296\u001b[0m ):\n\u001b[0;32m    297\u001b[0m     spec \u001b[38;5;241m=\u001b[39m CompositeSpec({out_keys[\u001b[38;5;241m0\u001b[39m]: spec})\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    300\u001b[0m     module,\n\u001b[1;32m--> 301\u001b[0m     \u001b[43mSafeProbabilisticModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    304\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\14385\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchrl\\modules\\tensordict_module\\probabilistic.py:154\u001b[0m, in \u001b[0;36mSafeProbabilisticModule.__init__\u001b[1;34m(self, in_keys, out_keys, spec, safe, default_interaction_mode, default_interaction_type, distribution_class, distribution_kwargs, return_log_prob, log_prob_key, cache_dist, n_empirical_estimate)\u001b[0m\n\u001b[0;32m    151\u001b[0m     spec_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(unravel_key_list(\u001b[38;5;28mlist\u001b[39m(spec\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m))))\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_keys \u001b[38;5;241m!=\u001b[39m out_keys:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspec keys and out_keys do not match, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m respectively\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spec \u001b[38;5;241m=\u001b[39m spec\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msafe \u001b[38;5;241m=\u001b[39m safe\n",
            "\u001b[1;31mRuntimeError\u001b[0m: spec keys and out_keys do not match, got: {'action', 'y', 'v', 'sample_log_prob', 'x'} and {'action', 'sample_log_prob'} respectively"
          ]
        }
      ],
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.size = size\n",
        "    self.action_dist = action_dist\n",
        "    self.action_spec = action_spec\n",
        "\n",
        "    self.input_layer = nn.LazyLinear(81)\n",
        "    self.flat = nn.Flatten()\n",
        "    self.dense_one = nn.LazyLinear(self.size)\n",
        "    self.dense_two = nn.LazyLinear(self.size)\n",
        "    self.output = nn.LazyLinear(self.action_dist)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.flat(x)\n",
        "    x = F.relu(self.input_layer(x))\n",
        "    x = F.relu(self.dense_one(x))\n",
        "    x = F.relu(self.dense_two(x))\n",
        "    x = F.relu(self.output(x))\n",
        "    x = torch.unflatten(x,-1,(self.action_spec))\n",
        "    x = F.softmax(x,-1)\n",
        "    return x  \n",
        "  \n",
        "ActorNetwork().forward(dummy_observation)\n",
        "Policy = TensorDictModule(module=ActorNetwork(), in_keys=[\"observation\"],out_keys=[\"probs\"])\n",
        "PolicyModule = ProbabilisticActor(module=Policy ,spec=env.action_spec,in_keys=[\"probs\"],\n",
        "                       distribution_class = OneHotCategorical,return_log_prob = True)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\14385\\AppData\\Local\\Temp\\ipykernel_15248\\2429551854.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  output = TensorDict({\"observation\" : torch.tensor(self.updatedBoard).float()})\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[45], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m Memory \u001b[38;5;241m=\u001b[39m ReplayBuffer(storage\u001b[38;5;241m=\u001b[39mLazyTensorStorage(max_size\u001b[38;5;241m=\u001b[39mframes),sampler\u001b[38;5;241m=\u001b[39mSamplerWithoutReplacement())\n\u001b[0;32m      2\u001b[0m Collector \u001b[38;5;241m=\u001b[39m SyncDataCollector(create_env_fn\u001b[38;5;241m=\u001b[39menv,policy\u001b[38;5;241m=\u001b[39mPolicyModule,frames_per_batch\u001b[38;5;241m=\u001b[39mframes,total_frames\u001b[38;5;241m=\u001b[39mtotal_frames)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mCollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m clear()\n",
            "File \u001b[1;32mc:\\Users\\14385\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchrl\\_utils.py:480\u001b[0m, in \u001b[0;36maccept_remote_rref_invocation.<locals>.unpack_rref_and_invoke_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _os_is_windows \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_distributed_rpc\u001b[38;5;241m.\u001b[39mPyRRef):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_value()\n\u001b[1;32m--> 480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\14385\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\14385\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchrl\\collectors\\collectors.py:1007\u001b[0m, in \u001b[0;36mSyncDataCollector.rollout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1006\u001b[0m     env_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle\n\u001b[1;32m-> 1007\u001b[0m env_output, env_next_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_and_maybe_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shuttle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m env_output:\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;66;03m# ad-hoc update shuttle\u001b[39;00m\n\u001b[0;32m   1011\u001b[0m     next_data \u001b[38;5;241m=\u001b[39m env_output\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\14385\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchrl\\envs\\common.py:2747\u001b[0m, in \u001b[0;36mEnvBase.step_and_maybe_reset\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   2705\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_and_maybe_reset\u001b[39m(\n\u001b[0;32m   2706\u001b[0m     \u001b[38;5;28mself\u001b[39m, tensordict: TensorDictBase\n\u001b[0;32m   2707\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TensorDictBase, TensorDictBase]:\n\u001b[0;32m   2708\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a step in the environment and (partially) resets it if needed.\u001b[39;00m\n\u001b[0;32m   2709\u001b[0m \n\u001b[0;32m   2710\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2745\u001b[0m \u001b[38;5;124;03m            is_shared=False)\u001b[39;00m\n\u001b[0;32m   2746\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2747\u001b[0m     tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2748\u001b[0m     \u001b[38;5;66;03m# done and truncated are in done_keys\u001b[39;00m\n\u001b[0;32m   2749\u001b[0m     \u001b[38;5;66;03m# We read if any key is done.\u001b[39;00m\n\u001b[0;32m   2750\u001b[0m     tensordict_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_mdp(tensordict)\n",
            "File \u001b[1;32mc:\\Users\\14385\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchrl\\envs\\common.py:1461\u001b[0m, in \u001b[0;36mEnvBase.step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m   1458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assert_tensordict_shape(tensordict)\n\u001b[0;32m   1459\u001b[0m next_preset \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m-> 1461\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1462\u001b[0m next_tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_proc_data(next_tensordict)\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_preset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1464\u001b[0m     \u001b[38;5;66;03m# tensordict could already have a \"next\" key\u001b[39;00m\n\u001b[0;32m   1465\u001b[0m     \u001b[38;5;66;03m# this could be done more efficiently by not excluding but just passing\u001b[39;00m\n\u001b[0;32m   1466\u001b[0m     \u001b[38;5;66;03m# the necessary keys\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[41], line 31\u001b[0m, in \u001b[0;36menvironment._step\u001b[1;34m(self, tensordict)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_step\u001b[39m(\u001b[38;5;28mself\u001b[39m,tensordict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorDictBase :\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m tensordict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdated,_ \u001b[38;5;241m=\u001b[39m \u001b[43mGame\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUpdated_board\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame \u001b[38;5;241m=\u001b[39m Game(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction)\n\u001b[0;32m     34\u001b[0m     reward \u001b[38;5;241m=\u001b[39m rewardFunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame)\u001b[38;5;241m.\u001b[39mrewardReturns()\n",
            "Cell \u001b[1;32mIn[37], line 16\u001b[0m, in \u001b[0;36mGame.Updated_board\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;66;03m#.tolist()[0] #tensor([[0, 1, 0]], dtype=torch.int32)\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     row,column,value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (row,column) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodifiableCells:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard[row][column] \u001b[38;5;241m=\u001b[39m value\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ],
      "source": [
        "Memory = ReplayBuffer(storage=LazyTensorStorage(max_size=frames),sampler=SamplerWithoutReplacement())\n",
        "Collector = SyncDataCollector(create_env_fn=env,policy=PolicyModule,frames_per_batch=frames,total_frames=total_frames)\n",
        "Collector.rollout()\n",
        "clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3nLd6x1AnDs"
      },
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.size = size\n",
        "    self.action_dist = action_dist\n",
        "    self.action_spec = action_spec\n",
        "\n",
        "    self.input_layer = nn.LazyLinear(self.size)\n",
        "    self.flat = nn.Flatten()\n",
        "    self.dense_one = nn.LazyLinear(self.size)\n",
        "    self.dense_two = nn.LazyLinear(self.size)\n",
        "    self.output = nn.LazyLinear(1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.flat(x)\n",
        "    x = F.relu(self.input_layer(x))\n",
        "    x = F.relu(self.dense_one(x))\n",
        "    x = F.relu(self.dense_two(x))\n",
        "    return self.output(x)\n",
        "\n",
        "Critic = Network_util(network=ValueNetwork())\n",
        "\n",
        "ValueModule = ValueOperator(module= Critic,in_keys=[\"observation\"])\n",
        "Advantage = GAE(gamma=gamma,lmbda=lmbda,value_network=ValueModule,average_gae=True,device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2tbJ05hLlkR"
      },
      "outputs": [],
      "source": [
        "loss = ClipPPOLoss(actor_network=PolicyModule,critic_network=ValueModule)\n",
        "optimizer = torch.optim.SGD(params=loss.parameters(),lr=l_rate,momentum=sdg_momentum )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMo6CiGGLZco"
      },
      "outputs": [],
      "source": [
        "class Training:\n",
        "    def __init__(self):\n",
        "        self.policy = ActorNetwork()\n",
        "        self.collector = Collector\n",
        "        self.memory = Memory\n",
        "        self.valuemodule = ValueModule\n",
        "        self.advantage = Advantage\n",
        "        self.lossfunction = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def save_logs(self):\n",
        "            log_dir = \"trainingData/\"  \n",
        "            self.writer = SummaryWriter(log_dir)\n",
        "\n",
        "    def save_weight(self):\n",
        "        path = \"trainingData/actor_100k.pth\"  \n",
        "        torch.save(self.policy.state_dict(),path)\n",
        "    \n",
        "    def fullyTrainedmodel(self):\n",
        "        path = \"trainingData/fullyTrainedmodel.pth\"\n",
        "        torch.save(self.policy.state_dict(),path)\n",
        "\n",
        "    def train(self,start : bool = None):\n",
        "        if start:\n",
        "            bestReward = -10\n",
        "            rewardHistory = deque(maxlen = 10)\n",
        "            self.save_logs()\n",
        "            for i,data_tensordict in tqdm(enumerate(self.collector),total = total_frames/frames):\n",
        "\n",
        "                for e in range(self.epochs):\n",
        "                    dat = self.advantage(data_tensordict)\n",
        "                    dat[\"advantage\"] = dat[\"advantage\"].unsqueeze(-1)\n",
        "                    self.memory.extend(dat)\n",
        "\n",
        "                    for n in range(total_frames//sub_frame):\n",
        "                        subdata = Memory.sample(sub_frame)\n",
        "                        loss_val = self.lossfunction(subdata.to(device))\n",
        "                        loss_value = (loss_val[\"loss_objective\"] + loss_val[\"loss_critic\"] + loss_val[\"loss_entropy\"])\n",
        "                        loss_value.backward()\n",
        "                        self.optimizer.step()\n",
        "                        self.optimizer.zero_grad()\n",
        "\n",
        "                self.writer.add_scalar(\"main/batch_number\",i)\n",
        "                self.writer.add_scalar(\"main/Advantage\",dat[\"advantage\"][0].item())\n",
        "                self.writer.add_scalar(\"main/Loss_sum\",loss_value.item())\n",
        "                self.writer.add_scalar(\"main/reward\",data_tensordict[\"next\"][\"reward\"][0].mean().item())\n",
        "                self.writer.add_scalar(\"main/reward_advantage\",dat[\"next\"][\"reward\"][0].mean().item())\n",
        "                self.writer.add_scalar(\"loss/Loss_entropy\",loss_val[\"loss_entropy\"].item())\n",
        "                self.writer.add_scalar(\"loss/Loss_critic\",loss_val[\"loss_critic\"].item())\n",
        "                self.writer.add_scalar(\"loss/Loss_objective\",loss_val[\"loss_objective\"].item())\n",
        "\n",
        "                currentReward = data_tensordict[\"next\"][\"reward\"][0].mean()\n",
        "                rewardHistory.append(currentReward)\n",
        "                averageReward = sum(rewardHistory)/len(rewardHistory)\n",
        "\n",
        "                if i % 10 == 0:\n",
        "                  if averageReward > bestReward:\n",
        "                    self.save_weight()\n",
        "                    bestReward = averageReward\n",
        "\n",
        "                self.fullyTrainedmodel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evzBJgPJnml1",
        "outputId": "afb8ac85-341b-4b30-858f-32c2cac057ed"
      },
      "outputs": [],
      "source": [
        "Training().train(start=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO+jAeQFBuHrZI2VDVdbRVk",
      "include_colab_link": true,
      "mount_file_id": "1_lC2ngW2272azpP9OPB1SB9FRya0fTmE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
