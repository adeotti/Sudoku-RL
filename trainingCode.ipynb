{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeotti/sudoku-env/blob/main/M9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_VxUuPOyB90G"
      },
      "outputs": [],
      "source": [
        "# python version 3.9.11 \n",
        "%pip install numpy==1.25.2 tensorboard==2.17.0 torchrl==0.4.0 gymnasium==0.29.1 tensordict==0.4.0\n",
        "\n",
        "from IPython.display import clear_output\n",
        "def clear():\n",
        "  clear_output(wait=False)\n",
        "\n",
        "import math,sys \n",
        "import torch\n",
        "\n",
        "clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "easyBoard = torch.tensor([\n",
        "    [0, 0, 0, 5, 3, 1, 0, 0, 0],\n",
        "    [0, 0, 0, 0, 4, 0, 3, 0, 1],\n",
        "    [1, 0, 0, 8, 0, 0, 0, 0, 0],\n",
        "    [0, 0, 4, 0, 0, 5, 6, 0, 0],\n",
        "    [0, 0, 3, 9, 0, 2, 1, 4, 0],\n",
        "    [6, 1, 5, 0, 7, 0, 0, 9, 8],\n",
        "    [0, 2, 0, 0, 9, 6, 0, 1, 0],\n",
        "    [0, 5, 7, 2, 0, 8, 0, 0, 6],\n",
        "    [0, 6, 1, 7, 5, 3, 0, 2, 4]])\n",
        "\n",
        "solution = torch.tensor([\n",
        "    [8, 4, 9, 5, 3, 1, 7, 6, 2],\n",
        "    [5, 7, 2, 6, 4, 9, 3, 8, 1],\n",
        "    [1, 3, 6, 8, 2, 7, 4, 5, 9],\n",
        "    [2, 9, 4, 1, 8, 5, 6, 7, 3],\n",
        "    [7, 8, 3, 9, 6, 2, 1, 4, 5],\n",
        "    [6, 1, 5, 3, 7, 4, 2, 9, 8],\n",
        "    [3, 2, 8, 7, 9, 5, 1, 6, 7],\n",
        "    [4, 5, 7, 2, 1, 8, 9, 3, 6],\n",
        "    [9, 6, 1, 7, 5, 3, 8, 2, 4]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Game and Utility class "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A8GSYA58P6SD"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class Board_specs:\n",
        "  size: tuple = (9,9)\n",
        "  low: int = 1\n",
        "  high: int = 9\n",
        "\n",
        "class Game:\n",
        "    def __init__(self,action = None):\n",
        "        self.board = easyBoard\n",
        "        self.action = action\n",
        "        self.reward = 0\n",
        "\n",
        "        self.modifiableCells = []\n",
        "        \n",
        "        for i,x in enumerate(self.board):\n",
        "            for y in range(Board_specs.high): \n",
        "                if x[y] == 0:\n",
        "                    self.modifiableCells.append((i,y))            \n",
        "\n",
        "    def Updated_board(self):\n",
        "        if self.action is not None:\n",
        "            row,column,value = self.action\n",
        "            if (row,column) in self.modifiableCells:\n",
        "                self.reward += 1\n",
        "\n",
        "                if value != self.board[row][column]:\n",
        "                    self.reward+= 1\n",
        "                    #x\n",
        "                    x = self.board[row].tolist()\n",
        "                    x.pop(column)\n",
        "              \n",
        "                    #y\n",
        "                    y = [element[column].item() for element in self.board]\n",
        "                    y.pop(row)\n",
        "                     \n",
        "                    #region\n",
        "                    n = int(math.sqrt(Board_specs.high))\n",
        "                    ix,iy = (self.action[0]//n)* n , (self.action[1]//n)* n\n",
        "                    region = torch.flatten(self.board[ix:ix+n , iy:iy+n]).tolist()\n",
        "\n",
        "                    local_row = row - ix\n",
        "                    local_col = column - iy\n",
        "                    action_index = local_row * n + local_col\n",
        "                    region_ = [num for idx, num in enumerate(region) if idx != action_index]\n",
        "\n",
        "                    sector = [x,y,region_]\n",
        "                    sector = [item for sublist in sector for item in sublist]\n",
        "\n",
        "                    sector_ = [element for element in sector if element !=0] # filtered the zeros\n",
        "\n",
        "                    if not value in sector_:\n",
        "                        self.board[row][column] = value\n",
        "                        self.reward +=10\n",
        "                    else:\n",
        "                        self.reward -= 5\n",
        "                    return self.board,self.reward\n",
        "                        \n",
        "                else :\n",
        "                    self.reward -= 5\n",
        "                    return self.board,self.reward\n",
        "\n",
        "            else:\n",
        "                self.reward -=10\n",
        "                return self.board,self.reward\n",
        "\n",
        "        return self.board,self.reward\n",
        "    \n",
        "    def gameEnd(self):\n",
        "        return torch.equal(solution,self.Updated_board()[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reward Function, Heuristic and the end of game function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchrl.envs import EnvBase\n",
        "from torchrl.data import BoundedTensorSpec,CompositeSpec\n",
        "from tensordict import TensorDictBase,TensorDict\n",
        "\n",
        "class environment(EnvBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.action = None\n",
        "        self.game = Game(self.action)\n",
        "        self.updatedBoard,self.reward = self.game.Updated_board()\n",
        "\n",
        "        self.action_spec = BoundedTensorSpec(\n",
        "            low=[[0,0,1]],\n",
        "            high=[[9,9,9]],\n",
        "            shape=torch.Size([1,3]),  \n",
        "            dtype=torch.int\n",
        "        )\n",
        "\n",
        "        self.observation_format = BoundedTensorSpec(\n",
        "            low=1.0,\n",
        "            high=9.0,\n",
        "            shape=(easyBoard).unsqueeze(0).shape,\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        self.observation_spec = CompositeSpec(observation = self.observation_format)\n",
        "\n",
        "    def _step(self,tensordict) -> TensorDictBase :\n",
        "        self.action = tensordict[\"action\"][0] # original shape -> tensor([[x, y, value]])\n",
        "        self.game = Game(self.action)\n",
        "        self.updated,self.reward = Game(self.action).Updated_board()\n",
        "        done = self.game.gameEnd()\n",
        "\n",
        "        output = TensorDict(\n",
        "            {\n",
        "                \"observation\" : self.updatedBoard.clone().detach().unsqueeze(0).float(),\n",
        "                \"reward\" : self.reward,\n",
        "                \"done\" : done\n",
        "            }\n",
        "        )\n",
        "        return output\n",
        "\n",
        "    def _reset(self,tensordict,**kwargs) -> TensorDictBase :  \n",
        "        output = TensorDict(\n",
        "            {\n",
        "                \"observation\" :  self.updatedBoard.clone().detach().unsqueeze(0).float()\n",
        "                }\n",
        "        )\n",
        "        return output\n",
        "\n",
        "    def _set_seed(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Mmh6rSNIIe",
        "outputId": "94c5da36-583b-4f5b-e402-6d941992511d"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "from tensordict.nn import TensorDictModule\n",
        "\n",
        "from torchrl.modules import ValueOperator,ProbabilisticActor\n",
        "from torchrl.objectives.value import GAE\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hypers\n",
        "l_rate = 0.01\n",
        "sdg_momentum = 0.9\n",
        "\n",
        "frames =  1000            # number of steps\n",
        "sub_frame = 50              # for the most inner loop of the training step\n",
        "total_frames = 100_000     # maximum steps\n",
        "\n",
        "gamma = 0.80\n",
        "lmbda = 0.99\n",
        "\n",
        "env = environment()\n",
        "dummy_observation = env._reset(None)[\"observation\"] \n",
        "\n",
        "@torch.no_grad()\n",
        "def weights_init(w):\n",
        "  if isinstance(w,(nn.Conv2d,nn.LazyConv2d,nn.LazyLinear)):\n",
        "    nn.init.kaiming_uniform(w.weight,mode=\"fan_in\",nonlinearity=\"relu\")\n",
        "    if w.bias is not None : nn.init.zeros_(w.bias)\n",
        "\n",
        "def networkInit(network : nn.Module):\n",
        "  network.to(device)\n",
        "  network.forward(dummy_observation)\n",
        "  network.apply(weights_init)\n",
        "  return network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "g3Sw6-yR8kH4"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
        "\n",
        "class Mask: \n",
        "  # This will alter the softmax distribution so value in [x,y,value] != 0 \n",
        "  def __init__(self):\n",
        "    self.newValue = -float(\"inf\")\n",
        "\n",
        "  def apply(self,tensor : torch.FloatTensor):\n",
        "    self.mask = torch.zeros_like(tensor,dtype=torch.bool)\n",
        "    self.mask[-1,-1,0] = True\n",
        "    tensor = tensor.masked_fill(mask=self.mask,value=self.newValue)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "class ActorNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.size = 81\n",
        "    self.outputShape = 27 # 3*9 = 27 haha\n",
        "    self.outputReshaped = (3,9)\n",
        "    self.mask = Mask()\n",
        "\n",
        "    self.input_layer = nn.LazyLinear(81)\n",
        "    self.flat = nn.Flatten()\n",
        "    self.dense_one = nn.LazyLinear(self.size)\n",
        "    self.dense_two = nn.LazyLinear(self.size)\n",
        "    self.output = nn.LazyLinear(self.outputShape)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.flat(x)\n",
        "    x = F.relu(self.input_layer(x))\n",
        "    x = F.relu(self.dense_one(x))\n",
        "    x = F.relu(self.dense_two(x))\n",
        "    x = F.relu(self.output(x))\n",
        "    x = torch.unflatten(x,-1,(self.outputReshaped))\n",
        "    x = self.mask.apply(x)\n",
        "    return F.softmax(x,-1)\n",
        "\n",
        "Actor = networkInit(network=ActorNetwork()) \n",
        "\n",
        "Policy = TensorDictModule(\n",
        "  module=Actor, \n",
        "  in_keys=[\"observation\"],\n",
        "  out_keys=[\"probs\"]\n",
        ")\n",
        "\n",
        "PolicyModule = ProbabilisticActor(\n",
        "  module=Policy ,\n",
        "  spec=env.action_spec,in_keys=[\"probs\"],\n",
        "  distribution_class = Categorical,\n",
        "  return_log_prob = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "Collector = SyncDataCollector(\n",
        "    create_env_fn=env,\n",
        "    policy=PolicyModule,\n",
        "    frames_per_batch=frames,\n",
        "    total_frames=total_frames,\n",
        ")\n",
        "Collector.rollout() \n",
        "clear()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "S3nLd6x1AnDs"
      },
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.size = 81\n",
        "    self.input_layer = nn.LazyLinear(self.size)\n",
        "    self.flat = nn.Flatten()\n",
        "    self.dense_one = nn.LazyLinear(self.size)\n",
        "    self.dense_two = nn.LazyLinear(self.size)\n",
        "    self.output = nn.LazyLinear(1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.flat(x)\n",
        "    x = F.relu(self.input_layer(x))\n",
        "    x = F.relu(self.dense_one(x))\n",
        "    x = F.relu(self.dense_two(x))\n",
        "    return self.output(x)\n",
        "\n",
        "Critic = networkInit(network=ValueNetwork())\n",
        "\n",
        "ValueModule = ValueOperator(\n",
        "  module= Critic,\n",
        "  in_keys=[\"observation\"]\n",
        ")\n",
        "\n",
        "Advantage = GAE(\n",
        "  gamma=gamma,\n",
        "  lmbda=lmbda,\n",
        "  value_network=ValueModule,\n",
        "  average_gae=True,\n",
        "  device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMo6CiGGLZco"
      },
      "outputs": [],
      "source": [
        "from torchrl.data import ReplayBuffer,SamplerWithoutReplacement,LazyTensorStorage\n",
        "from torchrl.objectives import ClipPPOLoss\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.distributions import kl_divergence\n",
        "\n",
        "class Training:\n",
        "    def __init__(self):\n",
        "        self.collector = Collector\n",
        "        self.epochs = 10\n",
        "        self.valuemodule = ValueModule\n",
        "        self.advantage = Advantage\n",
        "\n",
        "        self.policy = PolicyModule\n",
        "\n",
        "        self.lossfunction = ClipPPOLoss(\n",
        "            actor_network=PolicyModule,\n",
        "            critic_network=ValueModule,\n",
        "            entropy_coef=0.02\n",
        "        )\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            params=self.lossfunction.parameters(),\n",
        "            lr=l_rate\n",
        "        )\n",
        "        self.memory = ReplayBuffer(\n",
        "            storage=LazyTensorStorage(max_size=frames),\n",
        "            sampler=SamplerWithoutReplacement()\n",
        "        )\n",
        "\n",
        "    def save_logs(self):\n",
        "            log_dir = \"trainingData/new/\"  \n",
        "            self.writer = SummaryWriter(log_dir)\n",
        "\n",
        "    def save_weight(self):\n",
        "        path = \"trainingData/new/actor_100k.pth\"  \n",
        "        torch.save(self.policy.state_dict(),path)\n",
        "    \n",
        "    def fullyTrainedmodel(self):\n",
        "        path = \"trainingData/new/fullyTrainedmodel.pth\"\n",
        "        torch.save(self.policy.state_dict(),path)\n",
        "\n",
        "    def train(self,start : bool = None):\n",
        "        if start:\n",
        "            bestReward = -20\n",
        "            rewardHistory = deque(maxlen = 10)\n",
        "            self.save_logs()\n",
        "\n",
        "            for i,data_tensordict in tqdm(enumerate(self.collector),total = total_frames/frames):\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    oldist = Categorical(self.policy(data_tensordict[\"observation\"])[-1])\n",
        "                dat = self.advantage(data_tensordict)\n",
        "                dat[\"advantage\"] = dat[\"advantage\"].unsqueeze(-1)\n",
        "                self.memory.extend(dat)\n",
        "                # 1000 frame after rollout are uploaded to the memory and they will be destroyed after using \n",
        "                # the next 1000 frames will be uploaded and the cycles will continue \n",
        "                for _ in range(self.epochs):\n",
        "                    for _ in  range(frames//sub_frame):\n",
        "                        subdata = self.memory.sample(sub_frame)\n",
        "                        loss_val = self.lossfunction(subdata.to(device))\n",
        "                        loss_value = (\n",
        "                            loss_val[\"loss_objective\"] + \n",
        "                            loss_val[\"loss_critic\"] + \n",
        "                            loss_val[\"loss_entropy\"]\n",
        "                        )\n",
        "                        self.optimizer.zero_grad()\n",
        "                        loss_value.backward()\n",
        "                        self.optimizer.step()\n",
        "                        \n",
        "                        with torch.no_grad():\n",
        "                            newdist = Categorical(self.policy(dat[\"observation\"])[-1])\n",
        "                            kl = round(kl_divergence(oldist,newdist).mean().item(),3)\n",
        "                            if kl > 0.015:\n",
        "                                print(\"stopped : Kl > 0.015\")\n",
        "                                break\n",
        "\n",
        "                self.writer.add_scalar(\"main/batch_number\",i)\n",
        "                self.writer.add_scalar(\"main/Advantage\",dat[\"advantage\"][0].item())\n",
        "                self.writer.add_scalar(\"main/reward\",data_tensordict[\"next\"][\"reward\"][0].mean().item())\n",
        "                self.writer.add_scalar(\"main/KL\",kl)\n",
        "               \n",
        "                self.writer.add_scalar(\"loss/Entropy\",loss_val[\"loss_entropy\"].item())\n",
        "                self.writer.add_scalar(\"loss/Loss_sum\",loss_value.item())\n",
        "                self.writer.add_scalar(\"loss/Loss_entropy\",round(loss_val[\"loss_entropy\"].item(),4))\n",
        "                self.writer.add_scalar(\"loss/Loss_critic\",loss_val[\"loss_critic\"].item())\n",
        "                self.writer.add_scalar(\"loss/Loss_objective\",loss_val[\"loss_objective\"].item())\n",
        "\n",
        "                currentReward = data_tensordict[\"next\"][\"reward\"][0].mean()\n",
        "                rewardHistory.append(currentReward)\n",
        "                averageReward = sum(rewardHistory)/len(rewardHistory)\n",
        "                self.writer.add_scalar(\"main/Average reward\",averageReward)\n",
        "\n",
        "                if averageReward > bestReward:\n",
        "                    self.save_weight()\n",
        "                    bestReward = averageReward\n",
        "\n",
        "            self.fullyTrainedmodel() # saving at the end of the training just for safety \n",
        "\n",
        "Training().train(start=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO+jAeQFBuHrZI2VDVdbRVk",
      "include_colab_link": true,
      "mount_file_id": "1_lC2ngW2272azpP9OPB1SB9FRya0fTmE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
