{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from environment import environment\n",
        "import random,sys,gc,warnings\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import gymnasium \n",
        "from puzzle import easyBoard\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hypers\n",
        "batchSize = 10\n",
        "lr = 1e-4\n",
        "env = gymnasium.make(\"sudoku\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class mask: # altering softmax output so x and y = {0,8} and value = {1,9}\n",
        "  def __init__(self):\n",
        "    self.newValue = -float(\"inf\")\n",
        "\n",
        "  def apply(self,tensor : torch.FloatTensor):\n",
        "    self.mask = torch.zeros_like(tensor,dtype=torch.bool)\n",
        "    self.mask[0,-1] = True\n",
        "    self.mask[1,-1] = True\n",
        "    self.mask[-1,0] = True\n",
        "    tensor = tensor.masked_fill(mask=self.mask,value=self.newValue)\n",
        "    return tensor\n",
        " \n",
        "class network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.representation = nn.Sequential(\n",
        "      nn.LazyConv2d(16,3,1,0),\n",
        "       nn.ReLU(),\n",
        "       nn.LazyConv2d(16,3,1,0),\n",
        "       nn.ReLU() # torch.Size([32, 5, 5])\n",
        "    )\n",
        "    self.prediction = nn.Sequential(\n",
        "      nn.Conv2d(16,1,1,1),\n",
        "      nn.ReLU(),\n",
        "      nn.Flatten(1),\n",
        "      nn.LazyLinear(27),\n",
        "      nn.ReLU() # torch.Size([1, 25])\n",
        "    )\n",
        "    self.mask = mask() # for the softmax distribution mask \n",
        "    self.policy = nn.LazyLinear(27) \n",
        "    self.value = nn.LazyLinear(1)\n",
        "\n",
        "    self.dynamic = nn.Sequential(\n",
        "      nn.LazyConv2d(32,1,1,0),\n",
        "      nn.ReLU(),\n",
        "      nn.LazyConv2d(32,1,1,0)\n",
        "    )\n",
        "    self.next_hidden_state_layer = nn.LazyConv2d(32,1,1,0)\n",
        "    self.reward = nn.LazyLinear(1)\n",
        "\n",
        "    self.optim = torch.optim.Adam(self.parameters(),lr=lr)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = F.one_hot(x.to(torch.int64),num_classes=10).to(torch.float32)\n",
        "    x = self.representation(x)\n",
        "    x = self.prediction(x)\n",
        "\n",
        "    policy = self.policy(x)\n",
        "    softmax = F.softmax(self.mask.apply(policy.view(3,9)))  \n",
        "    value = self.value(x)\n",
        "    return softmax,value\n",
        "\n",
        "  def forward_static(self,hidden_state,action):\n",
        "    x = self.dynamic(hidden_state + action) # not a real implementation...\n",
        "    next_hidden_state = self.next_hidden_state_layer(x)\n",
        "    reward = self.reward(x)\n",
        "    return next_hidden_state,reward\n",
        "\n",
        "\n",
        "model = network()\n",
        "w,r = model.forward(easyBoard)\n",
        "\n",
        "Categorical(w).sample()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class collector:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "        \n",
        "    def rollout(self):\n",
        "        pass\n",
        "\n",
        "    def sample(self):\n",
        "        pass\n",
        "\n",
        "    def clear_memory():\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multiply(n=0,m=2,max=10,count = 0):\n",
        "    if count!= max:\n",
        "        print(f\"{n} X {m} = {n*m}\")\n",
        "        multiply(n=n+1,m=2,max=10,count=count+1)\n",
        "    \n",
        "multiply()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO+jAeQFBuHrZI2VDVdbRVk",
      "include_colab_link": true,
      "mount_file_id": "1_lC2ngW2272azpP9OPB1SB9FRya0fTmE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
