{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeotti/sudoku-env/blob/main/M9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from puzzle import easyBoard,solution\n",
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modifiableCells = []\n",
        "for i,x in enumerate(easyBoard):\n",
        "    for y in range(9): \n",
        "        if x[y] == 0: \n",
        "            modifiableCells.append((i,y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def actionCheck(action:tuple):\n",
        "    x,y,value = action\n",
        "    assert x in range(0,10), f\"{x} is too big or too small idk...\"\n",
        "    assert y in range(0,10), f\"{y} is too big or too small idk...\"\n",
        "    assert value in range(1,10), f\"{value} is too big or too small idk...\"\n",
        "\n",
        "def region(index:tuple|list,board: torch.Tensor): # return the region (row,column,block) of a cell\n",
        "    x,y = index\n",
        "    xlist = board[x].tolist()\n",
        "    xlist.pop(y)\n",
        "\n",
        "    ylist = [element[y].tolist() for element in board]\n",
        "    ylist.pop(x)\n",
        "\n",
        "    #block\n",
        "    n = int(math.sqrt(9))\n",
        "    ix,iy = (x//n)* n , (y//n)* n\n",
        "    block = torch.flatten(board[ix:ix+n , iy:iy+n]).tolist()\n",
        "    local_row = x - ix\n",
        "    local_col = y - iy\n",
        "    action_index = local_row * n + local_col\n",
        "    block_ = [num for idx, num in enumerate(block) if idx != action_index]\n",
        "\n",
        "    #output\n",
        "    Region = [xlist,ylist,block_]\n",
        "    Region = [item for sublist in Region for item in sublist]\n",
        "    return Region\n",
        "\n",
        "\n",
        "class solver: \n",
        "    def __init__(self,state:torch.Tensor,modCells:list):\n",
        "        self.board = state.clone()\n",
        "        self.solution = solution.clone()\n",
        "        self.modCells = modCells\n",
        "        self.maxStep = len(modCells)*3\n",
        "         \n",
        "    def domain(self,idx:tuple|list) -> list :\n",
        "        Region = region(idx,self.board)\n",
        "        Region = set([item for item in Region if item != 0]) \n",
        "\n",
        "        domain_ = set(range(1,10)) \n",
        "        TrueDomain = list(domain_ - Region)\n",
        "        return TrueDomain\n",
        "    \n",
        "    def collector(self):\n",
        "        queu = []\n",
        "        for element in self.modCells:\n",
        "            queu.append({element : self.domain(element)})\n",
        "        return queu\n",
        "    \n",
        "    def isSolvable(self) -> bool: \n",
        "        count = 0\n",
        "        while True:\n",
        "            self.__init__(self.board,self.modCells)\n",
        "            data = self.collector()\n",
        "            for dictt in data:\n",
        "                for k,v in dictt.items():\n",
        "                    if len(v) == 1:\n",
        "                        self.board[k] = v[0]\n",
        "            count+=1\n",
        "            if len(data) == 0:\n",
        "                break\n",
        "            else:\n",
        "                if count > self.maxStep:\n",
        "                    break\n",
        "\n",
        "        diff = (self.board == solution)\n",
        "        diff = (diff == True).sum().item()\n",
        "    \n",
        "        if diff == solution.numel(): # if all True cells = 81 :\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Env:\n",
        "    def __init__(self):\n",
        "        self.modifiableCells = modifiableCells.copy()\n",
        "\n",
        "    def step(self,action : tuple|list,state:torch.Tensor):\n",
        "        actionCheck(action)\n",
        "        self.action = action\n",
        "        x,y,value = self.action\n",
        "        reward = self.rewardFunction(action,state)\n",
        "        \n",
        "        if reward > 0:\n",
        "            state.squeeze(0)[x][y] = value\n",
        "        done = torch.equal(solution,state)  \n",
        "        return [\n",
        "                state, \\\n",
        "                torch.tensor([reward]),\\\n",
        "                torch.tensor([done]),  \\\n",
        "                torch.tensor([action])\n",
        "        ]\n",
        "           \n",
        "    def rewardFunction(self,action:tuple|list,board:torch.Tensor):\n",
        "        \"\"\" \n",
        "        This will call the solver method to check if the board is solvable after a cell is filled.\n",
        "        This fill a copy of the given board so the result here does not affect the original state\n",
        "        if the board is solvable then the index of the value (x,y) is removed from the list of modifiables cells\n",
        "        \"\"\"\n",
        "        reward = 0\n",
        "        x,y,value = action\n",
        "        board = board.squeeze(0).clone()\n",
        "        copyList = self.modifiableCells.copy()\n",
        "\n",
        "        board[x][y] = value\n",
        "    \n",
        "        if (x,y) not in copyList:\n",
        "            return 0\n",
        "        \n",
        "        copyList.remove((x,y))\n",
        "        Solver = solver(board.clone(),copyList)\n",
        "\n",
        "        diff = (board == solution) # the difference between the state and the solution\n",
        "        conflicts = (diff == False).sum().item() * 0.1\n",
        "\n",
        "        if Solver.isSolvable():\n",
        "            reward = round((conflicts + 0.5),2)\n",
        "            self.modifiableCells.remove((x,y))\n",
        "        else:\n",
        "            reward = -conflicts\n",
        "           \n",
        "        return reward \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "batchSize = 1\n",
        "lr = 0.001\n",
        "\n",
        "class Mask: \n",
        "  # This will alter the softmax distribution so value in [x,y,value] != 0 \n",
        "  def __init__(self):\n",
        "    self.newValue = -float(\"inf\")\n",
        "\n",
        "  def apply(self,tensor : torch.FloatTensor):\n",
        "    self.mask = torch.zeros_like(tensor,dtype=torch.bool)\n",
        "    self.mask[-1,-1,0] = True\n",
        "    tensor = tensor.masked_fill(mask=self.mask,value=self.newValue)\n",
        "    return tensor\n",
        "\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.batchsize = batchSize\n",
        "    self.action_dist = 27\n",
        "    self.action_spec = (3,9)\n",
        "    self.mask = Mask()\n",
        "\n",
        "    self.inputt = nn.LazyLinear(9)\n",
        "    self.conv1 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv2 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv3 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv4 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.output = nn.LazyLinear(self.action_dist)\n",
        "\n",
        "    self.optimizer = Adam(self.parameters(),lr = lr)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.inputt(x))\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = torch.flatten(x,1,2)\n",
        "    x = F.relu(self.output(x))\n",
        "    x = torch.unflatten(x,-1,(self.action_spec))\n",
        "    x = self.mask.apply(x)\n",
        "    return F.softmax(x,-1)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.batchsize = batchSize\n",
        "    self.input = nn.LazyLinear(9)\n",
        "    self.conv1 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv2 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv3 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv4 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "\n",
        "    self.optimizer = Adam(self.parameters(),lr = lr)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x = self.input(x)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.conv4(x) # -> shape [1,1,1]\n",
        "    return torch.flatten(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = Actor()\n",
        "a(torch.rand((1,9,9),dtype=torch.float))\n",
        "b = Critic()\n",
        "b(torch.rand((1,9,9),dtype=torch.float))\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class collector:\n",
        "    def __init__(self,totalFrame,batchSize):\n",
        "        assert totalFrame % batchSize == 0 , f\"TotalFrame / batchSize should yield 0\"\n",
        "        assert totalFrame < len(modifiableCells)*3 ,f\"The memory lenght should be less than an episodes\"\n",
        "        \n",
        "        self.state = easyBoard.clone().unsqueeze(0)\n",
        "        self.totalFrame = totalFrame\n",
        "        self.batchSize = batchSize\n",
        "        self.env = Env()\n",
        "        self.actor = Actor()\n",
        "        self.critic = Critic()\n",
        "        self.pointer = 0\n",
        "        self.reward = []\n",
        "        self.valueDAta = []\n",
        "        self.envData = []\n",
        "        self.networkData = []\n",
        "        \n",
        "    def rollout(self):\n",
        "        self.clear()\n",
        "        for _ in range(self.totalFrame):\n",
        "            value = self.critic.forward(self.state)\n",
        "            self.valueDAta.append(value)\n",
        "\n",
        "            dist = Categorical(self.actor.forward(self.state))\n",
        "            sample = dist.sample()\n",
        "            logProb = dist.log_prob(sample)\n",
        "            action = sample.tolist()[0]\n",
        "            self.networkData.append(logProb)\n",
        "\n",
        "            dataPoint = self.env.step(action,self.state)\n",
        "            self.envData.append(dataPoint)\n",
        "\n",
        "        for sublist,logs in zip(self.envData,self.networkData): # puting the logProb into the sublist of self.envData\n",
        "            sublist.append(logs)\n",
        "\n",
        "        for sublist,value in zip(self.envData,self.valueDAta): # putting the return in the sublists of self.envData\n",
        "            sublist.append(value)\n",
        "\n",
        "        random.shuffle(self.envData) # important here !\n",
        "\n",
        "        states = [item[0] for item in self.envData] \n",
        "        rewards = [item[1] for item in self.envData]\n",
        "        dones =  [item[2] for item in self.envData]\n",
        "        actions = [item[3] for item in self.envData]\n",
        "        oldProbs = [item[4] for item in self.envData]\n",
        "        values = [item[5] for item in self.envData]\n",
        "        \n",
        "        return states,rewards,dones,actions,oldProbs,values\n",
        "\n",
        "    def sample(self):\n",
        "        output = self.envData[self.pointer : self.pointer + self.batchSize]\n",
        "        self.pointer += self.batchSize\n",
        "        return output\n",
        "    \n",
        "    def clear(self):\n",
        "        self.networkData = []\n",
        "        self.envData = []\n",
        "        self.valueDAta = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = collector(4,2)\n",
        "data = env.rollout()\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        " \n",
        "{\\large\n",
        "\n",
        "\\begin{align}\n",
        "\n",
        "&\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
        "& \\hat{A_t} = \\delta_t + (\\gamma \\lambda)\\delta_{t+1}\\\\\n",
        "&GAE : \\hat{A_t} = \\delta_t + (\\gamma \\lambda)\\hat{A}_{t+1} \\\\[3em]\n",
        "\n",
        "&L_\\text{critic} = \\frac{1}{N} \\sum_t \\left( V_t - V_t^\\text{target} \\right)^2 \\\\\n",
        "& V_t^\\text{target} = \\hat{A_t} + V(s_t) \\\\[3em]\n",
        "\n",
        "&L^{CPI} = \\mathbb{\\hat{E_{t}}}\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta old}(a_t | s_t)}\\hat{A_t}  \\\\\n",
        "\\end{bmatrix} \\\\\n",
        "\n",
        "&\\hspace{2em} = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "r_t(\\theta)\\hat{A_t}  \\\\\n",
        "\\end{bmatrix} \\\\[1em]\n",
        "\n",
        "&L^{CLIP} = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "min(r_t(\\theta) \\hat{A_t},clip(r_t(\\theta),1-\\epsilon,1+\\epsilon) \\hat{A_t})\n",
        "\\end{bmatrix}\\\\[3em]\n",
        "\n",
        "&Total Loss : L_t(\\theta) = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "L_t^{CLIP}{\\theta} - c_1L_t^{critic}(\\theta)\n",
        "\\end{bmatrix}\\\\\n",
        "\n",
        "\\end{align}\n",
        "}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def GAE(rewards,values):\n",
        "    gamma = 0.9\n",
        "    llambda = 0.9\n",
        "    TDList = []\n",
        "    AtList = []\n",
        "    pointer = 1\n",
        "\n",
        "    for d in rewards:\n",
        "        try:\n",
        "            TD = d + gamma*values[pointer] - values[pointer-1]\n",
        "            TDList.append(TD)\n",
        "        except(IndexError):\n",
        "            TD = d + gamma - values[pointer-1]\n",
        "            TDList.append(TD)\n",
        "        pointer+=1\n",
        "\n",
        "    a_t = 0  \n",
        "    for td in reversed(TDList):\n",
        "        a_t = td + (gamma * llambda) * a_t\n",
        "        AtList.append(a_t)\n",
        "    AtList.reverse()\n",
        "    return AtList\n",
        "\n",
        "\n",
        "def criticLoss(advantages = None,values = None):\n",
        "    if not advantages is None and not values is None:\n",
        "        L = []\n",
        "        for item in range(len(advantages)):\n",
        "            vTarget = 0\n",
        "            vTarget = advantages[item] + values[item]\n",
        "            loss = (values[item] - vTarget)**2\n",
        "            L.append(loss)\n",
        "        L = sum(list(map(int,L)))/len(L)\n",
        "        return torch.tensor([L],dtype=torch.float)\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#torch.autograd.set_detect_anomaly(True)\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self,epochs,epsilon):\n",
        "        self.policy = Actor()\n",
        "        self.value = Critic()\n",
        "        self.memory = collector(totalFrame=4,batchSize=2)\n",
        "        self.valueLossfunction = criticLoss\n",
        "        self.epochs = epochs\n",
        "        self.epsilon = epsilon\n",
        "        self.c1 = 0.5 # the weight of the critic loss in the Total loss formula\n",
        "\n",
        "    def learn(self):\n",
        "        for i in tqdm(range(self.epochs),total=self.epochs):\n",
        "            states,rewards,dones,actions,oldProbs,values = self.memory.rollout()\n",
        "\n",
        "            #print(len(oldProbs))\n",
        "            advantages = GAE(rewards,values)\n",
        "        \n",
        "            criticLoss = self.valueLossfunction(advantages,values)\n",
        "            \n",
        "            newProbs = [] # computing the new log prob \n",
        "            state_action = list(zip(states,actions))\n",
        "            \n",
        "            for s,v in state_action:\n",
        "                probs = self.policy.forward(s)\n",
        "                dist = Categorical(probs)\n",
        "                np = dist.log_prob(v)\n",
        "                newProbs.append(np)\n",
        "\n",
        "            ratio = torch.stack(newProbs).exp()/torch.stack(oldProbs).exp()\n",
        "\n",
        "            actorLosslist = []\n",
        "            for i in range(len(advantages)):\n",
        "                ratioAdvantage = ratio*advantages[i]\n",
        "                clippedRatio = (torch.clamp(ratio,(1-self.epsilon),(1+self.epsilon))*advantages[i])\n",
        "                policyLoss = torch.min(ratioAdvantage,clippedRatio)\n",
        "                actorLosslist.append(policyLoss)\n",
        "            actorLoss = torch.mean(torch.stack(actorLosslist))\n",
        "            \n",
        "            totalLoss = actorLoss + self.c1*criticLoss # actorLoss + (weight critic loss * critic loss)\n",
        "        \n",
        "            self.policy.optimizer.zero_grad()\n",
        "            self.value.optimizer.zero_grad()\n",
        "            totalLoss.backward(retain_graph=True)\n",
        "            self.policy.optimizer.step()\n",
        "            self.value.optimizer.step()\n",
        "\n",
        "        self.memory.clear() # clear after each epochs\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z = Agent(epochs=4,epsilon=0.2)\n",
        "z.learn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rewards = [item[1] for item in data] # tensor\n",
        "values = [item[-1] for item in data] # tensor\n",
        "advantages = GAE(rewards,values) # tensor\n",
        "CLoss = criticLoss(advantages,values) # tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actions = actions = [item[3] for item in data]\n",
        "state = [item[0] for item in data] \n",
        "state_action = list(zip(state,actions))\n",
        "newProb = []\n",
        "a = Actor()\n",
        "\n",
        "for el in state_action:\n",
        "    probs = a.forward(el[0])\n",
        "    dist = Categorical(probs)\n",
        " \n",
        "    nP = dist.log_prob(el[1])\n",
        "    newProb.append(nP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "oldProbs = [item[4] for item in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ratio = torch.stack(newProb).exp()/torch.stack(oldProbs)\n",
        "epsilon = 0.2\n",
        "actorLoss = []\n",
        "for i in range(len(advantages)):\n",
        "    ratioAdvantage = ratio*advantages[i]\n",
        "    clippedRatio = (torch.clamp(ratio,(1-epsilon),(1+epsilon))*advantages[i])\n",
        "    policyLoss = torch.min(ratioAdvantage,clippedRatio)\n",
        "    actorLoss.append(policyLoss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "actorLoss = torch.mean(torch.stack(actorLoss))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO+jAeQFBuHrZI2VDVdbRVk",
      "include_colab_link": true,
      "mount_file_id": "1_lC2ngW2272azpP9OPB1SB9FRya0fTmE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
