{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeotti/sudoku-env/blob/main/M9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from puzzle import easyBoard,solution\n",
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "modifiableCells = []\n",
        "for i,x in enumerate(easyBoard):\n",
        "    for y in range(9): \n",
        "        if x[y] == 0: \n",
        "            modifiableCells.append((i,y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def actionCheck(action:tuple):\n",
        "    x,y,value = action\n",
        "    assert x in range(0,10), f\"{x} is too big or too small idk...\"\n",
        "    assert y in range(0,10), f\"{y} is too big or too small idk...\"\n",
        "    assert value in range(1,10), f\"{value} is too big or too small idk...\"\n",
        "\n",
        "def region(index:tuple|list,board: torch.Tensor): # return the region (row,column,block) of a cell\n",
        "    x,y = index\n",
        "    xlist = board[x].tolist()\n",
        "    xlist.pop(y)\n",
        "\n",
        "    ylist = [element[y].tolist() for element in board]\n",
        "    ylist.pop(x)\n",
        "\n",
        "    #block\n",
        "    n = int(math.sqrt(9))\n",
        "    ix,iy = (x//n)* n , (y//n)* n\n",
        "    block = torch.flatten(board[ix:ix+n , iy:iy+n]).tolist()\n",
        "    local_row = x - ix\n",
        "    local_col = y - iy\n",
        "    action_index = local_row * n + local_col\n",
        "    block_ = [num for idx, num in enumerate(block) if idx != action_index]\n",
        "\n",
        "    #output\n",
        "    Region = [xlist,ylist,block_]\n",
        "    Region = [item for sublist in Region for item in sublist]\n",
        "    return Region\n",
        "\n",
        "\n",
        "class solver: \n",
        "    def __init__(self,state:torch.Tensor,modCells:list):\n",
        "        self.board = state\n",
        "        self.solution = solution\n",
        "        self.modCells = modCells\n",
        "        self.maxStep = len(modCells)*3\n",
        "         \n",
        "    def domain(self,idx:tuple|list) -> list :\n",
        "        Region = region(idx,self.board)\n",
        "        Region = set([item for item in Region if item != 0]) \n",
        "\n",
        "        domain_ = set(range(1,10)) \n",
        "        TrueDomain = list(domain_ - Region)\n",
        "        return TrueDomain\n",
        "    \n",
        "    def collector(self):\n",
        "        queu = []\n",
        "        for element in self.modCells:\n",
        "            queu.append({element : self.domain(element)})\n",
        "        return queu\n",
        "    \n",
        "    def isSolvable(self) -> bool: \n",
        "        count = 0\n",
        "        while True:\n",
        "            self.__init__(self.board,self.modCells)\n",
        "            data = self.collector()\n",
        "            for dictt in data:\n",
        "                for k,v in dictt.items():\n",
        "                    if len(v) == 1:\n",
        "                        self.board[k] = v[0]\n",
        "            count+=1\n",
        "            if len(data) == 0:\n",
        "                break\n",
        "            else:\n",
        "                if count > self.maxStep:\n",
        "                    break\n",
        "\n",
        "        diff = (self.board == solution)\n",
        "        diff = (diff == True).sum().item()\n",
        "    \n",
        "        if diff == solution.numel(): # if all True cells = 81 :\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Env:\n",
        "    def __init__(self):\n",
        "        self.modifiableCells = modifiableCells.copy()\n",
        "\n",
        "    def step(self,action : tuple|list,state:torch.Tensor):\n",
        "        actionCheck(action)\n",
        "        self.action = action\n",
        "        x,y,value = self.action\n",
        "        reward = self.rewardFunction(action,state)\n",
        "        if reward > 0:\n",
        "            state[x][y] = value\n",
        "        done = torch.equal(solution,state)  \n",
        "        return [\n",
        "                state, \\\n",
        "                torch.tensor([reward],dtype=torch.float),\\\n",
        "                torch.tensor([done]),  \\\n",
        "                torch.tensor([action])\n",
        "        ]\n",
        "           \n",
        "    def rewardFunction(self,action:tuple|list,board:torch.Tensor):\n",
        "        \"\"\" \n",
        "        This will call the solver method to check if the board is solvable after a cell is filled.\n",
        "        This fill a copy of the given board so the result here does not affect the original state\n",
        "        if the board is solvable then the index of the value (x,y) is removed from the list of modifiables cells\n",
        "        \"\"\"\n",
        "        reward = 0\n",
        "        x,y,value = action\n",
        "        board = board.clone() \n",
        "        copyList = self.modifiableCells.copy()\n",
        "        board[x][y] = value\n",
        "    \n",
        "        if (x,y) not in copyList:\n",
        "            return 0\n",
        "        \n",
        "        copyList.remove((x,y))\n",
        "        Solver = solver(board.clone(),copyList)\n",
        "\n",
        "        diff = (board == solution) # the difference between the state and the solution\n",
        "        conflicts = (diff == False).sum().item() #* 0.1\n",
        "\n",
        "        if Solver.isSolvable():\n",
        "            reward = conflicts + 10 #round((conflicts + 5),2)  # 0.5\n",
        "            self.modifiableCells.remove((x,y))\n",
        "        else:\n",
        "            reward = -conflicts\n",
        "           \n",
        "        return reward \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "batchSize = 1\n",
        "lr = 0.001\n",
        "\n",
        "class Mask: \n",
        "  # This will alter the softmax distribution so value in [x,y,value] != 0 \n",
        "  def __init__(self):\n",
        "    self.newValue = -float(\"inf\")\n",
        "\n",
        "  def apply(self,tensor : torch.FloatTensor):\n",
        "    self.mask = torch.zeros_like(tensor,dtype=torch.bool)\n",
        "    self.mask[-1,-1,0] = True\n",
        "    tensor = tensor.masked_fill(mask=self.mask,value=self.newValue)\n",
        "    return tensor\n",
        "\n",
        "class Actor(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.batchsize = batchSize\n",
        "    self.action_dist = 27\n",
        "    self.action_spec = (3,9)\n",
        "    self.mask = Mask()\n",
        "\n",
        "    self.inputt = nn.LazyLinear(9)\n",
        "    self.conv1 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv2 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv3 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv4 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.output = nn.LazyLinear(self.action_dist)\n",
        "\n",
        "    self.optimizer = Adam(self.parameters(),lr = lr)\n",
        "    \n",
        "  def forward(self,x:torch.Tensor):\n",
        "    if not x.shape == torch.Size([1,9,9]) :\n",
        "      x = x.unsqueeze(0)\n",
        "    assert x.shape == torch.Size([1,9,9])\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.conv3(x)\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = torch.flatten(x,1,2)\n",
        "    x = F.relu(self.output(x))\n",
        "    x = torch.unflatten(x,-1,(self.action_spec))\n",
        "    x = self.mask.apply(x)\n",
        "    return F.softmax(x,-1)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.batchsize = batchSize\n",
        "    self.input = nn.LazyLinear(9)\n",
        "    self.conv1 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv2 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv3 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "    self.conv4 = nn.LazyConv2d(self.batchsize,(3,3))\n",
        "\n",
        "    self.optimizer = Adam(self.parameters(),lr = lr)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    if not x.shape == torch.Size([1,9,9]) :\n",
        "      x = x.unsqueeze(0)\n",
        "    assert x.shape == torch.Size([1,9,9])\n",
        "\n",
        "    x = self.input(x)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.conv4(x) # -> shape [1,1,1]\n",
        "    return torch.flatten(x)\n",
        "  \n",
        "Actor().forward(torch.rand((1,9,9),dtype=torch.float))\n",
        "Critic().forward(torch.rand((1,9,9),dtype=torch.float))\n",
        "clear_output()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from torch.distributions import Categorical\n",
        "import sys\n",
        "\n",
        "class collector:\n",
        "    def __init__(self,totalFrame,batchSize):\n",
        "        assert totalFrame % batchSize == 0 , f\"TotalFrame / batchSize should yield 0\"\n",
        "        assert totalFrame < len(modifiableCells)*3 ,f\"The memory lenght should be less than an episodes\"\n",
        "        \n",
        "        self.state = easyBoard.clone()\n",
        "        self.totalFrame = totalFrame\n",
        "        self.batchSize = batchSize\n",
        "        self.env = Env()\n",
        "        self.actor = Actor()\n",
        "        self.critic = Critic()\n",
        "\n",
        "        self.pointer = 0\n",
        "        self.updatedData = []\n",
        "        self.valueDAta = []\n",
        "        self.envData = []\n",
        "        self.networkData = []\n",
        "        \n",
        "    def rollout(self):\n",
        "        self.clear() # clearing before each epochs\n",
        "        for _ in range(self.totalFrame):\n",
        "            value = self.critic.forward(self.state.clone())\n",
        "            self.valueDAta.append(value)\n",
        "\n",
        "            dist = Categorical(self.actor.forward(self.state.clone()))\n",
        "            sample = dist.sample()\n",
        "            logProb = dist.log_prob(sample)\n",
        "            action = sample.tolist()[0]\n",
        "            self.networkData.append(logProb)\n",
        "\n",
        "            dataPoint = self.env.step(action,self.state)\n",
        "            self.envData.append(dataPoint)\n",
        "\n",
        "        for sublist,logs in zip(self.envData,self.networkData): # puting the logProb into the sublist of self.envData\n",
        "            sublist.append(logs)\n",
        "\n",
        "        for sublist,value in zip(self.envData,self.valueDAta): # putting the return in the sublists of self.envData\n",
        "            sublist.append(value)\n",
        "\n",
        "        random.shuffle(self.envData) # important here !\n",
        "        return self.envData  #[states,rewards,dones,actions,oldProbs,values]\n",
        "    \n",
        "    def extend(self,args):\n",
        "        self.updatedData = args\n",
        "\n",
        "    def sample(self):\n",
        "        output = self.updatedData[self.pointer : self.pointer + self.batchSize]\n",
        "        self.pointer += self.batchSize\n",
        "\n",
        "        states = [item[0] for item in output] \n",
        "        rewards = [item[1] for item in output]\n",
        "        dones =  [item[2] for item in output]\n",
        "        actions = [item[3] for item in output]\n",
        "        oldProbs = [item[4] for item in output]\n",
        "        values = [item[5] for item in output]\n",
        "        advanatages = [item[6] for item in output]\n",
        "\n",
        "        return states,rewards,dones,actions,oldProbs,values,advanatages\n",
        "    \n",
        "    def clear(self):\n",
        "        self.pointer = 0 \n",
        "        self.updatedData = []\n",
        "        self.networkData = []\n",
        "        self.envData = []\n",
        "        self.valueDAta = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        " \n",
        "{\\large\n",
        "\n",
        "\\begin{align}\n",
        "\n",
        "&\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
        "& \\hat{A_t} = \\delta_t + (\\gamma \\lambda)\\delta_{t+1}\\\\\n",
        "&GAE : \\hat{A_t} = \\delta_t + (\\gamma \\lambda)\\hat{A}_{t+1} \\\\[3em]\n",
        "\n",
        "&L_\\text{critic} = \\frac{1}{N} \\sum_t \\left( V_t - V_t^\\text{target} \\right)^2 \\\\\n",
        "& V_t^\\text{target} = \\hat{A_t} + V(s_t) \\\\[3em]\n",
        "\n",
        "&L^{CPI} = \\mathbb{\\hat{E_{t}}}\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta old}(a_t | s_t)}\\hat{A_t}  \\\\\n",
        "\\end{bmatrix} \\\\\n",
        "\n",
        "&\\hspace{2em} = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "r_t(\\theta)\\hat{A_t}  \\\\\n",
        "\\end{bmatrix} \\\\[1em]\n",
        "\n",
        "&L^{CLIP} = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "min(r_t(\\theta) \\hat{A_t},clip(r_t(\\theta),1-\\epsilon,1+\\epsilon) \\hat{A_t})\n",
        "\\end{bmatrix}\\\\[3em]\n",
        "\n",
        "&Total Loss : L_t(\\theta) = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "L_t^{CLIP}{\\theta} - c_1L_t^{critic}(\\theta)\n",
        "\\end{bmatrix}\\\\\n",
        "\n",
        "\\end{align}\n",
        "}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def GAE(data):\n",
        "    gamma = 0.9\n",
        "    llambda = 0.9\n",
        "    TDList = []\n",
        "    AtList = []\n",
        "    pointer = 1\n",
        "\n",
        "    rewards = [item[1] for item in data]\n",
        "    values = [item[5] for item in data]\n",
        "\n",
        "    for d in rewards:\n",
        "        try:\n",
        "            TD = d + gamma*values[pointer] - values[pointer-1]\n",
        "            TDList.append(TD)\n",
        "        except(IndexError):\n",
        "            TD = d + gamma - values[pointer-1]\n",
        "            TDList.append(TD)\n",
        "        pointer+=1\n",
        "\n",
        "    a_t = 0  \n",
        "    for td in reversed(TDList):\n",
        "        a_t = td + (gamma * llambda) * a_t\n",
        "        AtList.append(a_t)\n",
        "    AtList.reverse()\n",
        "\n",
        "    for sub,v in zip(data,AtList):\n",
        "        sub.append(v)\n",
        "    return data\n",
        "\n",
        "def criticLoss(advantages = None,values = None):\n",
        "    if not advantages is None and not values is None:\n",
        "        L = []\n",
        "        for item in range(len(advantages)):\n",
        "            vTarget = 0\n",
        "            vTarget = advantages[item] + values[item]\n",
        "            loss = (values[item] - vTarget)**2\n",
        "            L.append(loss)\n",
        "        L = torch.mean(torch.stack(L))\n",
        "        return L\n",
        "    return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "from tqdm import tqdm\n",
        "from torchviz import make_dot\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.policy = Actor()\n",
        "        self.value = Critic()\n",
        "\n",
        "        self.totalFrame = 100\n",
        "        self.batchSize = 20\n",
        "        self.epochs = 50\n",
        "        self.epsilon = 0.2\n",
        "        self.c1 = 0.5 # the weight of the critic loss in the Total loss formula\n",
        "\n",
        "        self.memory = collector(self.totalFrame,self.batchSize )\n",
        "        self.valueLossfunction = criticLoss\n",
        "        self.debug = False\n",
        "        self.writter = SummaryWriter(\"tdata/\")\n",
        "    \n",
        "    def save(self):\n",
        "        torch.save(self.policy.state_dict(),\"tdata/policy.pth\")\n",
        "\n",
        "    def learn(self):\n",
        "        for i in tqdm(range(self.epochs),total=self.epochs):\n",
        "            self.memory.clear() \n",
        "            roollout = self.memory.rollout()\n",
        "            advantage = GAE(roollout)\n",
        "            self.memory.extend(advantage)\n",
        "\n",
        "            for e in range(self.totalFrame//self.batchSize):\n",
        "                states,rewards,dones,actions,oldProbs,values,advantages = self.memory.sample()\n",
        "                r = torch.mean(torch.stack(rewards))\n",
        "                d = torch.flatten(torch.stack(dones))\n",
        "                if torch.any(d).item():\n",
        "                    self.save()\n",
        "                    print(\"weights saved\")\n",
        "                    break\n",
        "\n",
        "                criticLoss = self.valueLossfunction(advantages,values)\n",
        "                \n",
        "                newProbs = [] # computing the new log prob \n",
        "                state_action = list(zip(states,actions))\n",
        "                for s,a in state_action:\n",
        "                    probs = self.policy.forward(s)\n",
        "                    dist = Categorical(probs)\n",
        "                    np = dist.log_prob(a)\n",
        "                    newProbs.append(np)\n",
        "\n",
        "                ratio = torch.exp(torch.stack(newProbs)) / torch.exp(torch.stack(oldProbs))\n",
        "\n",
        "                actorLosslist = []\n",
        "                for i in range(len(advantages)):\n",
        "                    ratioAdvantage = ratio*advantages[i]\n",
        "                    clippedRatio = (torch.clamp(ratio,(1-self.epsilon),(1+self.epsilon))*advantages[i])\n",
        "                    policyLoss = torch.min(ratioAdvantage,clippedRatio)\n",
        "                    actorLosslist.append(policyLoss)\n",
        "                actorLoss = -torch.mean(torch.stack(actorLosslist))\n",
        "                \n",
        "                totalLoss = actorLoss + self.c1*criticLoss # actorLoss + (weight critic loss  * critic loss)\n",
        "\n",
        "                if self.debug: # print the computation graph, for debugging purposes\n",
        "                    graph = make_dot(totalLoss,params=dict(list(self.policy.named_parameters())))\n",
        "                    graph.render()\n",
        "                    self.debug = False\n",
        "\n",
        "                self.policy.optimizer.zero_grad()\n",
        "                self.value.optimizer.zero_grad()\n",
        "                totalLoss.backward(retain_graph=True)\n",
        "                self.policy.optimizer.step()\n",
        "                self.value.optimizer.step()\n",
        "\n",
        "            self.writter.add_scalar(\"main/Loss\",totalLoss)\n",
        "            self.writter.add_scalar(\"main/reward\",r)\n",
        "        \n",
        "        self.save()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z = Agent()\n",
        "z.learn()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO+jAeQFBuHrZI2VDVdbRVk",
      "include_colab_link": true,
      "mount_file_id": "1_lC2ngW2272azpP9OPB1SB9FRya0fTmE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
