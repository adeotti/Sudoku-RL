{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adeotti/sudoku-env/blob/main/M9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from puzzle import easyBoard,solution,testBoard,testSolution\n",
        "import torch\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modi(tensor) -> list: # returns modifiables cells index of a board and a mask\n",
        "    modlist = []\n",
        "    for i,x in enumerate(tensor):\n",
        "        for y in range(9): \n",
        "            if x[y] == 0: \n",
        "                modlist.append((i,y))\n",
        "    return modlist\n",
        "\n",
        "def region(index:tuple|list,board: torch.Tensor): # return the region (row,column,block) of a cell\n",
        "    x,y = index\n",
        "    xlist = board[x].tolist()\n",
        "    xlist.pop(y)\n",
        "\n",
        "    ylist = [element[y].tolist() for element in board]\n",
        "    ylist.pop(x)\n",
        "\n",
        "    #block\n",
        "    n = int(math.sqrt(9))\n",
        "    ix,iy = (x//n)* n , (y//n)* n\n",
        "    block = torch.flatten(board[ix:ix+n , iy:iy+n]).tolist()\n",
        "    local_row = x - ix\n",
        "    local_col = y - iy\n",
        "    action_index = local_row * n + local_col\n",
        "    block_ = [num for idx, num in enumerate(block) if idx != action_index]\n",
        "\n",
        "    #output\n",
        "    Region = [xlist,ylist,block_]\n",
        "    Region = [item for sublist in Region for item in sublist]\n",
        "    return Region\n",
        "\n",
        "class solver: \n",
        "    def __init__(self,state:torch.Tensor,modCells:list):\n",
        "        self.board = state\n",
        "        self.solution = solution\n",
        "        self.modCells = modCells\n",
        "        self.maxStep = len(modCells)*3\n",
        "         \n",
        "    def domain(self,idx:tuple|list) -> list :\n",
        "        Region = region(idx,self.board)\n",
        "        Region = set([item for item in Region if item != 0]) \n",
        "\n",
        "        domain_ = set(range(1,10)) \n",
        "        TrueDomain = list(domain_ - Region)\n",
        "        return TrueDomain\n",
        "    \n",
        "    def collector(self):\n",
        "        queu = []\n",
        "        for element in self.modCells:\n",
        "            queu.append({element : self.domain(element)})\n",
        "        return queu\n",
        "    \n",
        "    def isSolvable(self) -> bool: \n",
        "        count = 0\n",
        "        while True:\n",
        "            self.__init__(self.board,self.modCells)\n",
        "            data = self.collector()\n",
        "            for dictt in data:\n",
        "                for k,v in dictt.items():\n",
        "                    if len(v) == 1:\n",
        "                        self.board[k] = v[0]\n",
        "            count+=1\n",
        "            if len(data) == 0:\n",
        "                break\n",
        "            else:\n",
        "                if count > self.maxStep:\n",
        "                    break\n",
        "\n",
        "        diff = (self.board == solution)\n",
        "        diff = (diff == True).sum().item()\n",
        "    \n",
        "        if diff == solution.numel(): # if all True cells = 81 :\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "modifiableCells = modi(easyBoard)\n",
        "#maskModcells = modifiableCells.copy() # training mask "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Env:\n",
        "    def __init__(self):\n",
        "        self.modifiableCells = modifiableCells.copy()\n",
        "        self.solution = solution\n",
        "        self.state = easyBoard.clone()\n",
        "    \n",
        "    def reset(self):\n",
        "        self.state  = easyBoard.clone()\n",
        "        self.modifiableCells = modifiableCells.copy()\n",
        "\n",
        "    def step(self,action : tuple|list):#,state:torch.Tensor):\n",
        "        self.action = action\n",
        "        x,y,value = self.action\n",
        "        reward,conflicts = self.rewardFunction(action,self.state)\n",
        "        if reward > 0:\n",
        "            self.state[x][y] = value\n",
        "            self.modifiableCells.remove((x,y))\n",
        "        done = torch.equal(solution,self.state)  \n",
        "        return [\n",
        "                self.state, \\\n",
        "                torch.tensor([reward],dtype=torch.float),\\\n",
        "                torch.tensor([done]),  \\\n",
        "                torch.tensor([action]),\\\n",
        "                conflicts\n",
        "                ]\n",
        "           \n",
        "    def rewardFunction(self,action:tuple|list,board:torch.Tensor):\n",
        "        \"\"\" \n",
        "        This will call the solver method to check if the board is solvable after a cell is filled.\n",
        "        This fill a copy of the given board so the result here does not affect the original state\n",
        "        if the board is solvable then the index of the value (x,y) is removed from the list of modifiables cells\n",
        "        \"\"\"\n",
        "        reward = 0\n",
        "        x,y,value = action\n",
        "        board = board.clone() \n",
        "        copyList = self.modifiableCells.copy()\n",
        "        if not (x,y) in copyList:\n",
        "            diff = (board == self.solution) \n",
        "            conflicts = (diff == False).sum().item() \n",
        "            return 0,conflicts\n",
        "        board[x][y] = value\n",
        "        conflicts = ((board == self.solution) == False).sum().to(float) \n",
        "        copyList.remove((x,y)) # remove (x,y) before passing it to Solver\n",
        "        Solver = solver(board.clone(),copyList)\n",
        "        if Solver.isSolvable():\n",
        "            reward = (conflicts/2)*0.1 + 5 \n",
        "        else:\n",
        "            reward = -((conflicts/2)*0.1 + 5)\n",
        "        return reward,conflicts.floor()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "batchSize = 1\n",
        "lr = 0.0003\n",
        "\n",
        "class mask: # altering softmax output so x and y = {0,8} and value = {1,9}\n",
        "  def __init__(self):\n",
        "    self.newValue = -float(\"inf\")\n",
        "\n",
        "  def apply(self,tensor : torch.FloatTensor):\n",
        "    self.mask = torch.zeros_like(tensor,dtype=torch.bool)\n",
        "    self.mask[0,-1] = True\n",
        "    self.mask[1,-1] = True\n",
        "    self.mask[-1,0] = True\n",
        "    tensor = tensor.masked_fill(mask=self.mask,value=self.newValue)\n",
        "    return tensor\n",
        "\n",
        "class network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.LazyConv2d(1,(1,1))\n",
        "    self.conv2 = nn.LazyConv2d(1,(1,1))\n",
        "    self.conv3 = nn.LazyConv2d(1,(1,1))\n",
        "\n",
        "    self.linear1 = nn.LazyLinear(9)\n",
        "    self.linear2 = nn.LazyLinear(9)\n",
        "    self.linear3 = nn.LazyLinear(9)\n",
        "\n",
        "    self.policy_mask = mask()\n",
        "    self.policy_head = nn.LazyLinear(30)\n",
        "    self.value_head = nn.LazyLinear(1)\n",
        "    \n",
        "    self.optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = torch.flatten(x,start_dim=1)\n",
        "    x = self.linear1(x)\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    distibution = F.relu(self.policy_head(x)).reshape(3,10)\n",
        "    distibution = self.policy_mask.apply(distibution)\n",
        "    value = self.value_head(x)\n",
        "    return F.softmax(distibution,-1),value\n",
        "\n",
        "d = network()\n",
        "d.forward(torch.rand((1,9,9),dtype=torch.float))\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from torch.distributions import Categorical\n",
        "import gc\n",
        "\n",
        "class collector:\n",
        "    def __init__(self,totalFrame,batchSize):\n",
        "        assert totalFrame % batchSize == 0 , f\"TotalFrame / batchSize should yield 0\"\n",
        "        assert totalFrame < len(modifiableCells)*3 ,f\"The memory lenght should be less than an episodes\"\n",
        "        self.totalFrame = totalFrame\n",
        "        self.batchSize = batchSize\n",
        "        self.env = Env()\n",
        "        self.network = network()\n",
        "        self.pointer = 0\n",
        "        self.data = []\n",
        "     \n",
        "    def rollout(self):\n",
        "        self.clear() \n",
        "        while not len(self.data) == self.totalFrame : \n",
        "            if len(self.env.modifiableCells) < 5 :\n",
        "                self.env.reset()\n",
        "            softmax_dist,_value = self.network.forward(self.env.state.unsqueeze(0).clone())\n",
        "            dist = Categorical(softmax_dist)\n",
        "            sample = dist.sample()\n",
        "            _log_prob = dist.log_prob(sample)\n",
        "            action = sample.tolist()\n",
        "            assert len(action) == 3 , f\" action is {action}\"\n",
        "            x,y,_ = action\n",
        "            if (x,y) in self.env.modifiableCells:\n",
        "                _state,_reward,_done,_action,_conflicts = self.env.step(action,self.env.state)\n",
        "                self.data.append(_state,_reward,_action,_done,_conflicts,_log_prob,_value)\n",
        "        random.shuffle(self.data)  \n",
        "        return self.data  \n",
        "    \n",
        "    def extend(self,args):\n",
        "        self.data = args\n",
        "\n",
        "    def sample(self):\n",
        "        output = self.data[self.pointer : self.pointer + self.batchSize]\n",
        "        self.pointer += self.batchSize\n",
        "        states,rewards,actions,dones,conflicts,logs,values,advantages = zip(*output)\n",
        "        return states,rewards,dones,actions,logs,values,advantages,conflicts\n",
        "    \n",
        "    def clear(self):\n",
        "        gc.collect()\n",
        "        self.pointer = 0 \n",
        "        self.updatedData = []\n",
        "        self.networkData = []\n",
        "        self.envData = []\n",
        "        self.valueDAta = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        " \n",
        "{\\large\n",
        "\n",
        "\\begin{align}\n",
        "\n",
        "&\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
        "& \\hat{A_t} = \\delta_t + (\\gamma \\lambda)\\delta_{t+1}\\\\\n",
        "&GAE : \\hat{A_t} = \\delta_t + (\\gamma \\lambda)\\hat{A}_{t+1} \\\\[3em]\n",
        "\n",
        "&L_\\text{critic} = \\frac{1}{N} \\sum_t \\left( V_t - V_t^\\text{target} \\right)^2 \\\\\n",
        "& V_t^\\text{target} = \\hat{A_t} + V(s_t) \\\\[3em]\n",
        "\n",
        "&L^{CPI} = \\mathbb{\\hat{E_{t}}}\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta old}(a_t | s_t)}\\hat{A_t}  \\\\\n",
        "\\end{bmatrix} \\\\\n",
        "\n",
        "&\\hspace{2em} = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "r_t(\\theta)\\hat{A_t}  \\\\\n",
        "\\end{bmatrix} \\\\[1em]\n",
        "\n",
        "&L^{CLIP} = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "min(r_t(\\theta) \\hat{A_t},clip(r_t(\\theta),1-\\epsilon,1+\\epsilon) \\hat{A_t})\n",
        "\\end{bmatrix}\\\\[3em]\n",
        "\n",
        "&Total Loss : L_t(\\theta) = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "L_t^{CLIP}{\\theta} - c_1L_t^{critic}(\\theta)\n",
        "\\end{bmatrix}\\\\\n",
        "\n",
        "\\end{align}\n",
        "}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def GAE(data):\n",
        "    gamma = 0.99\n",
        "    llambda = 0.99\n",
        "    TDList = []\n",
        "    AtList = []\n",
        "    pointer = 1\n",
        "\n",
        "    rewards = [item[1] for item in data]\n",
        "    values = [item[5] for item in data]\n",
        "\n",
        "    for d in rewards:\n",
        "        try:\n",
        "            TD = d + gamma*values[pointer] - values[pointer-1]\n",
        "            TDList.append(TD)\n",
        "        except(IndexError):\n",
        "            TD = d + gamma - values[pointer-1]\n",
        "            TDList.append(TD)\n",
        "        pointer+=1\n",
        "\n",
        "    a_t = 0  \n",
        "    for td in reversed(TDList):\n",
        "        a_t = td + (gamma * llambda) * a_t\n",
        "        AtList.append(a_t)\n",
        "    AtList.reverse()\n",
        "\n",
        "    for sub,v in zip(data,AtList):\n",
        "        sub.append(v)\n",
        "    return data\n",
        "\n",
        "def criticLoss(advantages = None,values = None):\n",
        "    if not advantages is None and not values is None:\n",
        "        L = []\n",
        "        for item in range(len(advantages)):\n",
        "            vTarget = 0\n",
        "            vTarget = advantages[item] + values[item]\n",
        "            loss = (values[item] - vTarget)**2\n",
        "            L.append(loss)\n",
        "        L = torch.mean(torch.stack(L))\n",
        "        return L\n",
        "    return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self):\n",
        "        self.network = network()\n",
        "        self.env = Env()\n",
        "        self.totalFrame = 20\n",
        "        self.batchSize = 20\n",
        "        self.epochs = 100\n",
        "        self.epsilon = 0.2\n",
        "        self.c1 = 0.5 # Weight of the critic loss in the total loss \n",
        "        self.memory = collector(self.totalFrame,self.batchSize )\n",
        "        self.valueLossfunction = criticLoss\n",
        "        self.writter = SummaryWriter(\"./data/\")\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.network.state_dict(),\"./data/policy.pth\")\n",
        "\n",
        "    def learn(self):\n",
        "        for i in tqdm(range(self.epochs),total=self.epochs):\n",
        "            self.memory.clear() \n",
        "            roollout = self.memory.rollout()\n",
        "            advantage = GAE(roollout)\n",
        "            self.memory.extend(advantage)\n",
        "            for _ in range(1): # minibatches\n",
        "                states,rewards,dones,actions,oldProbs,values,advantages,conf = self.memory.sample()\n",
        "                for _ in range(10): # epochs optim\n",
        "                    conf = torch.mean(torch.stack(conf)).floor()\n",
        "                    r = torch.mean(torch.stack(rewards)) \n",
        "                    criticLoss = self.valueLossfunction(advantages,values)\n",
        "                    #c = torch.mean(torch.pow(v - vtarget))\n",
        "                    newProbs = [] # new log prob \n",
        "                    for s,a in zip(states,actions):\n",
        "                        probs = self.network.forward(s)\n",
        "                        dist = Categorical(probs)\n",
        "                        np = dist.log_prob(a)\n",
        "                        newProbs.append(np)\n",
        "                    ratio = torch.exp(torch.stack(newProbs)) / torch.exp(torch.stack(oldProbs))\n",
        "                    actorLosslist = []\n",
        "                    for i in range(len(advantages)):\n",
        "                        ratioAdvantage = ratio*advantages[i]\n",
        "                        clippedRatio = (torch.clamp(ratio,(1-self.epsilon),(1+self.epsilon))*advantages[i])\n",
        "                        policyLoss = torch.min(ratioAdvantage,clippedRatio)\n",
        "                        actorLosslist.append(policyLoss)\n",
        "                    actorLoss = -torch.mean(torch.stack(actorLosslist))\n",
        "                    totalLoss = actorLoss + self.c1*criticLoss # actorLoss + (weight critic loss  * critic loss\n",
        "                    self.network.optimizer.zero_grad()\n",
        "                    totalLoss.backward(retain_graph=True)\n",
        "                    self.network.optimizer.step()\n",
        "            self.writter.add_scalar(\"main/conflicts\",conf)\n",
        "            self.writter.add_scalar(\"main/Loss\",totalLoss)\n",
        "            self.writter.add_scalar(\"main/reward\",r)\n",
        "        self.save()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "z = Agent()\n",
        "z.learn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir \"./\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO+jAeQFBuHrZI2VDVdbRVk",
      "include_colab_link": true,
      "mount_file_id": "1_lC2ngW2272azpP9OPB1SB9FRya0fTmE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
