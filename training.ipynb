{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from main import environment\n",
        "import random,sys,gc,warnings\n",
        "from IPython.display import clear_output\n",
        "import gymnasium as gym \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.autograd.set_detect_anomaly(True)\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make(\"sudoku\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hypers\n",
        "batchSize = 200\n",
        "lr = 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class mask: # altering softmax output so x and y = {0,8} and value = {1,9}\n",
        "  def __init__(self):\n",
        "    self.newValue = -float(\"inf\")\n",
        "\n",
        "  def apply(self,tensor : torch.FloatTensor):\n",
        "    self.mask = torch.zeros_like(tensor,dtype=torch.bool)\n",
        "    self.mask[0,-1] = True\n",
        "    self.mask[1,-1] = True\n",
        "    self.mask[-1,0] = True\n",
        "    tensor = tensor.masked_fill(mask=self.mask,value=self.newValue)\n",
        "    return tensor\n",
        "\n",
        "class network(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.LazyConv2d(1,(1,1))\n",
        "    self.conv2 = nn.LazyConv2d(1,(1,1))\n",
        "    self.conv3 = nn.LazyConv2d(1,(1,1))\n",
        "\n",
        "    self.linear1 = nn.LazyLinear(9)\n",
        "    self.linear2 = nn.LazyLinear(9)\n",
        "    self.linear3 = nn.LazyLinear(9)\n",
        "\n",
        "    self.policy_mask = mask()\n",
        "    self.policy_head = nn.LazyLinear(30)\n",
        "    self.value_head = nn.LazyLinear(1)\n",
        "    \n",
        "    self.optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.conv2(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = torch.flatten(x,start_dim=1)\n",
        "    x = self.linear1(x)\n",
        "    x = F.relu(self.linear2(x))\n",
        "    x = self.linear3(x)\n",
        "    distibution = F.relu(self.policy_head(x)).reshape(3,10)\n",
        "    distibution = self.policy_mask.apply(distibution)\n",
        "    value = self.value_head(x)\n",
        "    return F.softmax(distibution,-1),value\n",
        "\n",
        "d = network()\n",
        "d.forward(torch.rand((1,9,9),dtype=torch.float))\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class collector:\n",
        "    def __init__(self,totalFrame,batchSize):\n",
        "        assert totalFrame % batchSize == 0 , f\"TotalFrame / batchSize should yield 0\"\n",
        "        assert totalFrame < len(modifiableCells)*3 ,f\"The memory lenght should be less than an episodes\"\n",
        "        self.totalFrame = totalFrame\n",
        "        self.env = Env()\n",
        "        self.network = network()\n",
        "        self.pointer = 0\n",
        "        self.data = []\n",
        "        self.gamma = 0.99\n",
        "        self.lambdaa = 0.99\n",
        "     \n",
        "    def rollout(self):\n",
        "        self.clear() \n",
        "        while not len(self.data) == self.totalFrame : \n",
        "            if len(self.env.modifiableCells) < 5 :\n",
        "                self.env.reset()\n",
        "            softmax_dist,_value = self.network.forward(self.env.state.unsqueeze(0).clone())\n",
        "            dist = Categorical(softmax_dist)\n",
        "            sample = dist.sample()\n",
        "            _log_prob = dist.log_prob(sample)\n",
        "            action = sample.tolist()\n",
        "            assert len(action) == 3 , f\" action is {action}, problem during rollout\"\n",
        "            x,y,_ = action\n",
        "            if (x,y) in self.env.modifiableCells:\n",
        "                _state,_reward,_done,_action,_conflicts = self.env.step(action)\n",
        "                self.data.append([_state,_reward,_action,_done,_conflicts,_log_prob,_value])\n",
        "        \n",
        "        # compute advantage\n",
        "        _,_rewards,_,_,_,_,_value = zip(*self.data)\n",
        "        re = torch.stack(_rewards,dim=-1).squeeze()\n",
        "        val = torch.cat((torch.stack(_value,dim=-1).squeeze(),torch.tensor([0])),dim=-1)\n",
        "        advantage = torch.zeros_like(re)\n",
        "        gae = 0\n",
        "        for n in reversed(range(len(re))):\n",
        "            td = re[n] + self.gamma * val[n+1] - val[n]\n",
        "            gae = td + self.gamma * self.lambdaa * gae\n",
        "            advantage[n] = gae\n",
        "    \n",
        "        for data,advaaa in zip(self.data,torch.flip(advantage,dims=[-1])):\n",
        "            data.append(advaaa)\n",
        "        random.shuffle(self.data)  \n",
        "     \n",
        "    def sample(self,minibatches):\n",
        "        output = self.data[self.pointer : self.pointer + minibatches]\n",
        "        self.pointer += minibatches\n",
        "        states,rewards,actions,dones,conflicts,logs,values,advantages = zip(*output)\n",
        "        return states,rewards,dones,actions,logs,values,advantages,conflicts\n",
        "    \n",
        "    def clear(self):\n",
        "        self.pointer = 0 \n",
        "        self.data = []\n",
        "        gc.collect()\n",
        "\n",
        "\n",
        "t = collector(4,1)\n",
        "t.rollout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class sudoku:\n",
        "    def __init__(self):\n",
        "        self.network = network()\n",
        "        self.env = Env()\n",
        "        self.totalFrame = 10#20\n",
        "        self.memory = collector(self.totalFrame,self.batchSize )\n",
        "        self.writter = SummaryWriter(\"./\")\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.network.state_dict(),\"./policy.pth\")\n",
        "\n",
        "    def learn(self):\n",
        "        for i in tqdm(range(self.epochs),total=self.epochs):\n",
        "            self.memory.clear() \n",
        "            self.memory.rollout()\n",
        "\n",
        "            for p in range(1): # minibatches\n",
        "                states,rewards,dones,actions,oldProbs,values,advantages,conf = self.memory.sample(5)\n",
        "                \n",
        "                for y in range(5): # epochs optim\n",
        "                    con = torch.mean(torch.stack(conf)).floor()\n",
        "                    r = torch.mean(torch.stack(rewards)) \n",
        "                    ad = torch.stack(advantages,dim=-1).squeeze()\n",
        "                    val = torch.stack(values,dim=-1).squeeze()\n",
        "                    \n",
        "                    vtarget = ad + val\n",
        "                    critic_loss = torch.mean(torch.pow((val-vtarget),2))\n",
        "                    # \n",
        "                    sta = torch.stack(states,dim=0)\n",
        "                    act = torch.stack(actions,dim=0)\n",
        "                    probs,_ = self.network.forward(sta)\n",
        "                    dist = Categorical(probs)\n",
        "                    np = dist.log_prob(act)\n",
        "                    ratio = torch.exp(np) / torch.exp(torch.stack(oldProbs))\n",
        "                    actorLosslist = []\n",
        "                    # policy loss\n",
        "                    for i in range(len(advantages)):\n",
        "                        ratioAdvantage = ratio*val[i]\n",
        "                        clippedRatio = (torch.clamp(ratio,(1-self.epsilon),(1+self.epsilon))*val[i])\n",
        "                        policyLoss = torch.min(ratioAdvantage,clippedRatio)\n",
        "                        actorLosslist.append(policyLoss)\n",
        "                    actorLoss = -torch.mean(torch.stack(actorLosslist))\n",
        "                \n",
        "                    totalLoss = actorLoss + self.c1*critic_loss # actorLoss + (weight critic loss  * critic loss\n",
        "                    self.network.optimizer.zero_grad()\n",
        "                    totalLoss.backward(retain_graph=True)\n",
        "                    self.network.optimizer.step()\n",
        "\n",
        "            self.writter.add_scalar(\"main/conflicts\",con)\n",
        "            self.writter.add_scalar(\"main/Loss\",totalLoss)\n",
        "            self.writter.add_scalar(\"main/reward\",r)\n",
        "\n",
        "        self.save()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir \"./\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "{\\small\n",
        "\n",
        "\\begin{align}\n",
        "\n",
        "&\\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
        "& \\hat{A_t} = \\delta_t + (\\gamma \\lambda)\\delta_{t+1}\\\\\n",
        "&GAE : \\hat{A_t} = \\delta_t + (\\gamma \\lambda)\\hat{A}_{t+1} \\\\[3em]\n",
        "\n",
        "&L_\\text{critic} = \\frac{1}{N} \\sum_t \\left( V_t - V_t^\\text{target} \\right)^2 \\\\\n",
        "& V_t^\\text{target} = \\hat{A_t} + V(s_t) \\\\[3em]\n",
        "\n",
        "&L^{CPI} = \\mathbb{\\hat{E_{t}}}\n",
        "\\begin{bmatrix}\n",
        "\\frac{\\pi_{\\theta}(a_t | s_t)}{\\pi_{\\theta old}(a_t | s_t)}\\hat{A_t}  \\\\\n",
        "\\end{bmatrix} \\\\\n",
        "\n",
        "&\\hspace{2em} = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "r_t(\\theta)\\hat{A_t}  \\\\\n",
        "\\end{bmatrix} \\\\[1em]\n",
        "\n",
        "&L^{CLIP} = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "min(r_t(\\theta) \\hat{A_t},clip(r_t(\\theta),1-\\epsilon,1+\\epsilon) \\hat{A_t})\n",
        "\\end{bmatrix}\\\\[3em]\n",
        "\n",
        "&Total Loss : L_t(\\theta) = \\mathbb{\\hat{E_t}}\n",
        "\\begin{bmatrix}\n",
        "L_t^{CLIP}{\\theta} - c_1L_t^{critic}(\\theta)\n",
        "\\end{bmatrix}\\\\\n",
        "\n",
        "\\end{align}\n",
        "}\n",
        "$$\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyO+jAeQFBuHrZI2VDVdbRVk",
      "include_colab_link": true,
      "mount_file_id": "1_lC2ngW2272azpP9OPB1SB9FRya0fTmE",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
